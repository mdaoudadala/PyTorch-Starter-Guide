{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[39,40,41], [1,2,3], [4,5,6], [3,4,5], [9,10,11], [200,201,202], [123,124,125]]\n",
    "y = [1,1,1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_fake_in = 100\n",
    "dim_fake_out = 3\n",
    "dim_real_in = 3\n",
    "dim_real_out = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 #len(x) \n",
    "batch_no = int(len(x)/batch_size)\n",
    "num_epoch = 1000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.dis = nn.Sequential(\n",
    "            nn.Linear(dim_real_in, 100),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, dim_real_out), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dis(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(dim_fake_in, 100),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, dim_fake_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.gen(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = discriminator()\n",
    "G = generator()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0529, -0.3957,  1.6574, -2.5071, -0.0315, -0.7634,  2.1508,  0.3674,\n",
      "         0.0926, -1.4116,  2.5305,  0.8315, -1.8872,  1.9796,  0.0060,  0.8756,\n",
      "         0.3273,  0.9504,  1.4729,  0.1864,  1.4700, -0.1253, -1.1068,  0.7024,\n",
      "        -0.2209,  1.4558, -0.9564,  0.1660, -0.4627,  1.1469, -1.1244,  0.1984,\n",
      "        -0.2962, -0.6126, -1.1484,  0.5382,  0.6646,  0.6597,  0.0182, -0.7468,\n",
      "         1.1215, -1.3836,  0.6570,  0.0889, -0.2235,  1.2361,  1.0872, -1.1625,\n",
      "        -0.3709,  0.7862, -0.8034,  0.5779,  0.7875,  1.6914,  0.3653,  0.1121,\n",
      "        -0.2596,  0.5853, -0.6683,  0.5010,  0.4279, -0.6140,  2.2870, -2.3149,\n",
      "        -0.2425,  0.1778,  1.5344, -0.1409,  0.5770,  1.2436,  0.5671,  0.3686,\n",
      "         0.8840, -0.2378,  1.5025, -0.0378,  0.2002, -1.3917,  0.0058,  0.0865,\n",
      "        -0.1214, -0.0242, -0.1541,  1.3804,  0.6238, -0.6165, -1.2166,  0.1069,\n",
      "        -0.7375,  0.5667, -0.9235, -0.8794, -1.1791, -0.4146,  0.9183,  1.3234,\n",
      "        -1.3080,  0.6995,  1.2027, -0.3909])\n",
      "tensor([0.1829, 0.0532, 0.0743], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = Variable(torch.randn(dim_fake_in).float())\n",
    "print (z)\n",
    "fake_seq = G(z)\n",
    "print (fake_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([39., 40., 41.])\n",
      "tensor([0.4889], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "z = Variable(torch.tensor(x[0]).float())\n",
    "print (z)\n",
    "fake_seq = D(z)\n",
    "print (fake_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1000], d_loss: 1.167511, g_loss: 0.696956 D real: 0.624698, D fake: 0.501935\n",
      "Epoch [100/1000], d_loss: 0.574665, g_loss: 0.893894 D real: 0.982255, D fake: 0.426938\n",
      "Epoch [200/1000], d_loss: 0.303686, g_loss: 0.936474 D real: 0.999342, D fake: 0.261421\n",
      "Epoch [300/1000], d_loss: 0.385130, g_loss: 1.182739 D real: 0.999867, D fake: 0.319547\n",
      "Epoch [400/1000], d_loss: 0.536960, g_loss: 0.953010 D real: 0.999944, D fake: 0.415445\n",
      "Epoch [500/1000], d_loss: 0.338195, g_loss: 1.102803 D real: 0.999599, D fake: 0.286658\n",
      "Epoch [600/1000], d_loss: 0.412744, g_loss: 0.860997 D real: 0.995265, D fake: 0.335020\n",
      "Epoch [700/1000], d_loss: 0.348111, g_loss: 1.197859 D real: 0.997260, D fake: 0.292040\n",
      "Epoch [800/1000], d_loss: 0.595424, g_loss: 1.271223 D real: 0.997920, D fake: 0.447522\n",
      "Epoch [900/1000], d_loss: 0.498784, g_loss: 0.730515 D real: 0.997114, D fake: 0.390974\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "for epoch in range(num_epoch):\n",
    "    \"\"\"\n",
    "    for i in range(batch_no):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs = Variable(torch.tensor(x_train[start:end]).float())\n",
    "        labels = Variable(torch.tensor(y_train[start:end]).long())\n",
    "    \"\"\"\n",
    "        \n",
    "    for i in range(len(x)):\n",
    "        real_seq = Variable(torch.tensor(x[i]).float())\n",
    "        real_label = Variable(torch.ones(y[i]).float())\n",
    "        fake_label = Variable(torch.zeros(y[i]).float())\n",
    "        \n",
    "        # ===============train discriminator\n",
    "        \n",
    "        # compute loss of real_seq\n",
    "        real_out = D(real_seq)\n",
    "        d_loss_real = criterion(real_out, real_label)\n",
    "        real_scores = real_out  # closer to 1 means better\n",
    "\n",
    "        # compute loss of fake_seq\n",
    "        z = Variable(torch.randn(dim_fake_in).float())\n",
    "        fake_seq = G(z)\n",
    "        fake_out = D(fake_seq)\n",
    "        d_loss_fake = criterion(fake_out, fake_label)\n",
    "        fake_scores = fake_out  # closer to 0 means better\n",
    "\n",
    "        # bp and optimize\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # ===============train generator\n",
    "        \n",
    "        # compute loss of fake_seq\n",
    "        z = Variable(torch.randn(dim_fake_in).float())\n",
    "        fake_seq = G(z)\n",
    "        output = D(fake_seq)\n",
    "        g_loss = criterion(output, real_label)\n",
    "\n",
    "        # bp and optimize\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "            print('Epoch [{}/{}], d_loss: {:.6f}, g_loss: {:.6f} '\n",
    "              'D real: {:.6f}, D fake: {:.6f}'.format(epoch, num_epoch, d_loss.item(), g_loss.item(),\n",
    "                  real_scores.data.mean(), fake_scores.data.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0479, 2.8769, 3.8868], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G(Variable(torch.randn(dim_fake_in).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7534, 2.7657, 3.7440], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G(Variable(torch.randn(dim_fake_in).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0094], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D(Variable(torch.tensor([2,113,107]).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4227], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D(Variable(torch.tensor([1,2,3]).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9990], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D(Variable(torch.tensor([200,201,202]).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
