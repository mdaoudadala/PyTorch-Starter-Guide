{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/aaditkapoor1201/iris-classification-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set numpy seed for reproducibility\n",
    "np.random.seed(seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f1   f2   f3   f4      species\n",
       "0  5.1  3.5  1.4  0.2  Iris-setosa\n",
       "1  4.9  3.0  1.4  0.2  Iris-setosa\n",
       "2  4.7  3.2  1.3  0.2  Iris-setosa\n",
       "3  4.6  3.1  1.5  0.2  Iris-setosa\n",
       "4  5.0  3.6  1.4  0.2  Iris-setosa"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../data/iris.csv\", sep=',', header=None)\n",
    "dataset.columns = ['f1', 'f2', 'f3', 'f4', 'species']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f1   f2   f3   f4  species\n",
       "0  5.1  3.5  1.4  0.2        0\n",
       "1  4.9  3.0  1.4  0.2        0\n",
       "2  4.7  3.2  1.3  0.2        0\n",
       "3  4.6  3.1  1.5  0.2        0\n",
       "4  5.0  3.6  1.4  0.2        0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset[\"label\"] = dataset[\"label\"].astype('category')\n",
    "#dataset['label_cat'] = dataset['label'].cat.codes.astype('float64')\n",
    "#dataset = dataset[['f1', 'f2', 'f3', 'f4', 'label_cat']]\n",
    "dataset.loc[dataset.species=='Iris-setosa', 'species'] = 0\n",
    "dataset.loc[dataset.species=='Iris-versicolor', 'species'] = 1\n",
    "dataset.loc[dataset.species=='Iris-virginica', 'species'] = 2\n",
    "\n",
    "#dataset = pd.get_dummies(dataset, prefix=['label'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1         float64\n",
       "f2         float64\n",
       "f3         float64\n",
       "f4         float64\n",
       "species      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 150. Train size: 120 - Test size: 30\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_size = int(len(dataset) * train_size)\n",
    "np.random.shuffle(dataset)\n",
    "train, test = dataset[:train_size,:], dataset[train_size:,:]\n",
    "print ('Dataset size: {0}. Train size: {1} - Test size: {2}'.format(len(dataset), len(train), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and set data into numpy nd.array\n",
    "x_train = train[:,:4]\n",
    "x_test = test[:,:4]\n",
    "y_train = train[:,4]\n",
    "y_test = test[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 2., 1., 0., 2., 1., 0., 0., 1., 2., 0., 1., 2., 2., 2., 0.,\n",
       "       0., 1., 0., 0., 2., 0., 2., 0., 0., 0., 2., 2., 0., 2., 2., 0., 0.,\n",
       "       1., 1., 2., 0., 0., 1., 1., 0., 2., 2., 2., 2., 2., 1., 0., 0., 2.,\n",
       "       0., 0., 1., 1., 1., 1., 2., 1., 2., 0., 2., 1., 0., 0., 2., 1., 2.,\n",
       "       2., 0., 1., 1., 2., 0., 2., 1., 1., 0., 2., 2., 0., 0., 1., 1., 2.,\n",
       "       0., 0., 1., 0., 1., 2., 0., 2., 0., 0., 1., 0., 0., 1., 2., 1., 1.,\n",
       "       1., 0., 0., 1., 2., 0., 0., 1., 1., 1., 2., 1., 1., 1., 2., 0., 0.,\n",
       "       1.])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(x_train) # 50\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "size_hidden = 100\n",
    "n_feature = 4\n",
    "n_output = 3\n",
    "batch_no = int(len(x_train) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    # define nn\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "        X = self.fc3(X)\n",
    "        X = self.softmax(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_feature, size_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(size_hidden, size_hidden),\n",
    "    torch.nn.Linear(size_hidden, n_output),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss:  1.1241374015808105\n",
      "Epoch 2 loss:  1.1141802072525024\n",
      "Epoch 3 loss:  1.1038709878921509\n",
      "Epoch 4 loss:  1.0938069820404053\n",
      "Epoch 5 loss:  1.0845600366592407\n",
      "Epoch 6 loss:  1.076492428779602\n",
      "Epoch 7 loss:  1.069683313369751\n",
      "Epoch 8 loss:  1.0639945268630981\n",
      "Epoch 9 loss:  1.0591870546340942\n",
      "Epoch 10 loss:  1.0550187826156616\n",
      "Epoch 11 loss:  1.0512880086898804\n",
      "Epoch 12 loss:  1.047844409942627\n",
      "Epoch 13 loss:  1.0445818901062012\n",
      "Epoch 14 loss:  1.0414268970489502\n",
      "Epoch 15 loss:  1.0383312702178955\n",
      "Epoch 16 loss:  1.0352617502212524\n",
      "Epoch 17 loss:  1.032198429107666\n",
      "Epoch 18 loss:  1.0291260480880737\n",
      "Epoch 19 loss:  1.0260354280471802\n",
      "Epoch 20 loss:  1.022922396659851\n",
      "Epoch 21 loss:  1.0197843313217163\n",
      "Epoch 22 loss:  1.016621708869934\n",
      "Epoch 23 loss:  1.0134360790252686\n",
      "Epoch 24 loss:  1.010230541229248\n",
      "Epoch 25 loss:  1.00700843334198\n",
      "Epoch 26 loss:  1.0037754774093628\n",
      "Epoch 27 loss:  1.0005381107330322\n",
      "Epoch 28 loss:  0.9973028898239136\n",
      "Epoch 29 loss:  0.9940769076347351\n",
      "Epoch 30 loss:  0.9908674955368042\n",
      "Epoch 31 loss:  0.9876800179481506\n",
      "Epoch 32 loss:  0.9845197796821594\n",
      "Epoch 33 loss:  0.9813926219940186\n",
      "Epoch 34 loss:  0.9783021211624146\n",
      "Epoch 35 loss:  0.9752511978149414\n",
      "Epoch 36 loss:  0.9722433686256409\n",
      "Epoch 37 loss:  0.9692805409431458\n",
      "Epoch 38 loss:  0.9663651585578918\n",
      "Epoch 39 loss:  0.963498055934906\n",
      "Epoch 40 loss:  0.9606803059577942\n",
      "Epoch 41 loss:  0.9579119086265564\n",
      "Epoch 42 loss:  0.9551941752433777\n",
      "Epoch 43 loss:  0.9525271058082581\n",
      "Epoch 44 loss:  0.9499103426933289\n",
      "Epoch 45 loss:  0.9473450779914856\n",
      "Epoch 46 loss:  0.9448301792144775\n",
      "Epoch 47 loss:  0.9423650503158569\n",
      "Epoch 48 loss:  0.9399492144584656\n",
      "Epoch 49 loss:  0.937580943107605\n",
      "Epoch 50 loss:  0.9352604150772095\n",
      "Epoch 51 loss:  0.9329865574836731\n",
      "Epoch 52 loss:  0.9307593703269958\n",
      "Epoch 53 loss:  0.9285770654678345\n",
      "Epoch 54 loss:  0.9264395236968994\n",
      "Epoch 55 loss:  0.9243456721305847\n",
      "Epoch 56 loss:  0.9222933053970337\n",
      "Epoch 57 loss:  0.9202827215194702\n",
      "Epoch 58 loss:  0.9183121919631958\n",
      "Epoch 59 loss:  0.9163811802864075\n",
      "Epoch 60 loss:  0.9144889712333679\n",
      "Epoch 61 loss:  0.9126336574554443\n",
      "Epoch 62 loss:  0.9108151793479919\n",
      "Epoch 63 loss:  0.909031093120575\n",
      "Epoch 64 loss:  0.9072821736335754\n",
      "Epoch 65 loss:  0.9055658578872681\n",
      "Epoch 66 loss:  0.9038821458816528\n",
      "Epoch 67 loss:  0.9022300243377686\n",
      "Epoch 68 loss:  0.9006076455116272\n",
      "Epoch 69 loss:  0.8990150094032288\n",
      "Epoch 70 loss:  0.8974507451057434\n",
      "Epoch 71 loss:  0.8959147334098816\n",
      "Epoch 72 loss:  0.8944053053855896\n",
      "Epoch 73 loss:  0.8929224014282227\n",
      "Epoch 74 loss:  0.891465425491333\n",
      "Epoch 75 loss:  0.8900330066680908\n",
      "Epoch 76 loss:  0.8886239528656006\n",
      "Epoch 77 loss:  0.887238085269928\n",
      "Epoch 78 loss:  0.8858746290206909\n",
      "Epoch 79 loss:  0.8845334649085999\n",
      "Epoch 80 loss:  0.8832135796546936\n",
      "Epoch 81 loss:  0.8819152116775513\n",
      "Epoch 82 loss:  0.8806372880935669\n",
      "Epoch 83 loss:  0.8793782591819763\n",
      "Epoch 84 loss:  0.878138542175293\n",
      "Epoch 85 loss:  0.8769170045852661\n",
      "Epoch 86 loss:  0.8757133483886719\n",
      "Epoch 87 loss:  0.874527633190155\n",
      "Epoch 88 loss:  0.8733585476875305\n",
      "Epoch 89 loss:  0.8722057938575745\n",
      "Epoch 90 loss:  0.8710691332817078\n",
      "Epoch 91 loss:  0.8699483275413513\n",
      "Epoch 92 loss:  0.8688426613807678\n",
      "Epoch 93 loss:  0.8677517771720886\n",
      "Epoch 94 loss:  0.8666756749153137\n",
      "Epoch 95 loss:  0.865613579750061\n",
      "Epoch 96 loss:  0.864564836025238\n",
      "Epoch 97 loss:  0.8635304570198059\n",
      "Epoch 98 loss:  0.8625085353851318\n",
      "Epoch 99 loss:  0.8614996075630188\n",
      "Epoch 100 loss:  0.8605027794837952\n",
      "Epoch 101 loss:  0.8595184683799744\n",
      "Epoch 102 loss:  0.8585454821586609\n",
      "Epoch 103 loss:  0.8575843572616577\n",
      "Epoch 104 loss:  0.8566341996192932\n",
      "Epoch 105 loss:  0.8556953072547913\n",
      "Epoch 106 loss:  0.8547664880752563\n",
      "Epoch 107 loss:  0.8538484573364258\n",
      "Epoch 108 loss:  0.8529399633407593\n",
      "Epoch 109 loss:  0.8520423173904419\n",
      "Epoch 110 loss:  0.8511537909507751\n",
      "Epoch 111 loss:  0.850274920463562\n",
      "Epoch 112 loss:  0.8494061231613159\n",
      "Epoch 113 loss:  0.8485460877418518\n",
      "Epoch 114 loss:  0.8476945757865906\n",
      "Epoch 115 loss:  0.8468524217605591\n",
      "Epoch 116 loss:  0.8460184335708618\n",
      "Epoch 117 loss:  0.8451927304267883\n",
      "Epoch 118 loss:  0.8443752527236938\n",
      "Epoch 119 loss:  0.8435657024383545\n",
      "Epoch 120 loss:  0.8427644968032837\n",
      "Epoch 121 loss:  0.841970682144165\n",
      "Epoch 122 loss:  0.8411847949028015\n",
      "Epoch 123 loss:  0.8404061198234558\n",
      "Epoch 124 loss:  0.8396350145339966\n",
      "Epoch 125 loss:  0.8388713598251343\n",
      "Epoch 126 loss:  0.8381140828132629\n",
      "Epoch 127 loss:  0.8373634815216064\n",
      "Epoch 128 loss:  0.83661949634552\n",
      "Epoch 129 loss:  0.8358826041221619\n",
      "Epoch 130 loss:  0.835152268409729\n",
      "Epoch 131 loss:  0.8344277143478394\n",
      "Epoch 132 loss:  0.8337094783782959\n",
      "Epoch 133 loss:  0.83299720287323\n",
      "Epoch 134 loss:  0.8322907090187073\n",
      "Epoch 135 loss:  0.8315898776054382\n",
      "Epoch 136 loss:  0.8308943510055542\n",
      "Epoch 137 loss:  0.8302043080329895\n",
      "Epoch 138 loss:  0.8295198082923889\n",
      "Epoch 139 loss:  0.8288402557373047\n",
      "Epoch 140 loss:  0.8281656503677368\n",
      "Epoch 141 loss:  0.8274961113929749\n",
      "Epoch 142 loss:  0.8268312215805054\n",
      "Epoch 143 loss:  0.8261715173721313\n",
      "Epoch 144 loss:  0.8255165219306946\n",
      "Epoch 145 loss:  0.8248655796051025\n",
      "Epoch 146 loss:  0.8242197036743164\n",
      "Epoch 147 loss:  0.8235776424407959\n",
      "Epoch 148 loss:  0.8229402899742126\n",
      "Epoch 149 loss:  0.8223077654838562\n",
      "Epoch 150 loss:  0.8216795325279236\n",
      "Epoch 151 loss:  0.8210552930831909\n",
      "Epoch 152 loss:  0.8204352259635925\n",
      "Epoch 153 loss:  0.8198188543319702\n",
      "Epoch 154 loss:  0.8192061185836792\n",
      "Epoch 155 loss:  0.8185973763465881\n",
      "Epoch 156 loss:  0.8179924488067627\n",
      "Epoch 157 loss:  0.817391037940979\n",
      "Epoch 158 loss:  0.8167934417724609\n",
      "Epoch 159 loss:  0.8161987066268921\n",
      "Epoch 160 loss:  0.8156077265739441\n",
      "Epoch 161 loss:  0.8150199055671692\n",
      "Epoch 162 loss:  0.8144359588623047\n",
      "Epoch 163 loss:  0.8138548135757446\n",
      "Epoch 164 loss:  0.8132770657539368\n",
      "Epoch 165 loss:  0.8127020597457886\n",
      "Epoch 166 loss:  0.8121309280395508\n",
      "Epoch 167 loss:  0.811562180519104\n",
      "Epoch 168 loss:  0.8109966516494751\n",
      "Epoch 169 loss:  0.8104342222213745\n",
      "Epoch 170 loss:  0.8098742961883545\n",
      "Epoch 171 loss:  0.809316873550415\n",
      "Epoch 172 loss:  0.8087627291679382\n",
      "Epoch 173 loss:  0.8082107901573181\n",
      "Epoch 174 loss:  0.8076618313789368\n",
      "Epoch 175 loss:  0.8071159720420837\n",
      "Epoch 176 loss:  0.8065720796585083\n",
      "Epoch 177 loss:  0.8060314059257507\n",
      "Epoch 178 loss:  0.805493175983429\n",
      "Epoch 179 loss:  0.8049572110176086\n",
      "Epoch 180 loss:  0.8044233918190002\n",
      "Epoch 181 loss:  0.8038920760154724\n",
      "Epoch 182 loss:  0.8033629655838013\n",
      "Epoch 183 loss:  0.8028361797332764\n",
      "Epoch 184 loss:  0.8023112416267395\n",
      "Epoch 185 loss:  0.801788866519928\n",
      "Epoch 186 loss:  0.8012681603431702\n",
      "Epoch 187 loss:  0.8007500767707825\n",
      "Epoch 188 loss:  0.8002336025238037\n",
      "Epoch 189 loss:  0.7997190356254578\n",
      "Epoch 190 loss:  0.7992064952850342\n",
      "Epoch 191 loss:  0.7986961007118225\n",
      "Epoch 192 loss:  0.7981871366500854\n",
      "Epoch 193 loss:  0.797680675983429\n",
      "Epoch 194 loss:  0.7971763610839844\n",
      "Epoch 195 loss:  0.7966736555099487\n",
      "Epoch 196 loss:  0.7961729764938354\n",
      "Epoch 197 loss:  0.7956741452217102\n",
      "Epoch 198 loss:  0.7951772212982178\n",
      "Epoch 199 loss:  0.7946822643280029\n",
      "Epoch 200 loss:  0.7941890358924866\n",
      "Epoch 201 loss:  0.7936968207359314\n",
      "Epoch 202 loss:  0.7932068705558777\n",
      "Epoch 203 loss:  0.7927184700965881\n",
      "Epoch 204 loss:  0.792231023311615\n",
      "Epoch 205 loss:  0.7917462587356567\n",
      "Epoch 206 loss:  0.7912625074386597\n",
      "Epoch 207 loss:  0.7907810211181641\n",
      "Epoch 208 loss:  0.7903010845184326\n",
      "Epoch 209 loss:  0.7898222208023071\n",
      "Epoch 210 loss:  0.7893447875976562\n",
      "Epoch 211 loss:  0.7888692021369934\n",
      "Epoch 212 loss:  0.7883943915367126\n",
      "Epoch 213 loss:  0.7879214882850647\n",
      "Epoch 214 loss:  0.787449300289154\n",
      "Epoch 215 loss:  0.7869789600372314\n",
      "Epoch 216 loss:  0.7865097522735596\n",
      "Epoch 217 loss:  0.7860421538352966\n",
      "Epoch 218 loss:  0.7855756282806396\n",
      "Epoch 219 loss:  0.7851104140281677\n",
      "Epoch 220 loss:  0.7846462726593018\n",
      "Epoch 221 loss:  0.7841834425926208\n",
      "Epoch 222 loss:  0.7837221026420593\n",
      "Epoch 223 loss:  0.7832616567611694\n",
      "Epoch 224 loss:  0.7828027009963989\n",
      "Epoch 225 loss:  0.7823448181152344\n",
      "Epoch 226 loss:  0.781887948513031\n",
      "Epoch 227 loss:  0.7814319133758545\n",
      "Epoch 228 loss:  0.7809774279594421\n",
      "Epoch 229 loss:  0.7805238962173462\n",
      "Epoch 230 loss:  0.7800714373588562\n",
      "Epoch 231 loss:  0.7796201109886169\n",
      "Epoch 232 loss:  0.7791697978973389\n",
      "Epoch 233 loss:  0.778720498085022\n",
      "Epoch 234 loss:  0.7782723903656006\n",
      "Epoch 235 loss:  0.7778252959251404\n",
      "Epoch 236 loss:  0.7773793339729309\n",
      "Epoch 237 loss:  0.7769343852996826\n",
      "Epoch 238 loss:  0.7764907479286194\n",
      "Epoch 239 loss:  0.7760475873947144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240 loss:  0.7756052613258362\n",
      "Epoch 241 loss:  0.7751644849777222\n",
      "Epoch 242 loss:  0.7747243046760559\n",
      "Epoch 243 loss:  0.7742855548858643\n",
      "Epoch 244 loss:  0.7738475203514099\n",
      "Epoch 245 loss:  0.7734108567237854\n",
      "Epoch 246 loss:  0.7729748487472534\n",
      "Epoch 247 loss:  0.7725395560264587\n",
      "Epoch 248 loss:  0.7721054553985596\n",
      "Epoch 249 loss:  0.771671712398529\n",
      "Epoch 250 loss:  0.7712393999099731\n",
      "Epoch 251 loss:  0.7708075642585754\n",
      "Epoch 252 loss:  0.7703769207000732\n",
      "Epoch 253 loss:  0.7699469327926636\n",
      "Epoch 254 loss:  0.7695177793502808\n",
      "Epoch 255 loss:  0.7690893411636353\n",
      "Epoch 256 loss:  0.7686624526977539\n",
      "Epoch 257 loss:  0.7682355642318726\n",
      "Epoch 258 loss:  0.7678101062774658\n",
      "Epoch 259 loss:  0.7673851847648621\n",
      "Epoch 260 loss:  0.7669610977172852\n",
      "Epoch 261 loss:  0.7665374875068665\n",
      "Epoch 262 loss:  0.7661149501800537\n",
      "Epoch 263 loss:  0.7656928300857544\n",
      "Epoch 264 loss:  0.7652714252471924\n",
      "Epoch 265 loss:  0.764851450920105\n",
      "Epoch 266 loss:  0.7644315958023071\n",
      "Epoch 267 loss:  0.7640134692192078\n",
      "Epoch 268 loss:  0.7635959982872009\n",
      "Epoch 269 loss:  0.7631790041923523\n",
      "Epoch 270 loss:  0.7627630233764648\n",
      "Epoch 271 loss:  0.7623475193977356\n",
      "Epoch 272 loss:  0.761932373046875\n",
      "Epoch 273 loss:  0.7615183591842651\n",
      "Epoch 274 loss:  0.7611052393913269\n",
      "Epoch 275 loss:  0.7606919407844543\n",
      "Epoch 276 loss:  0.7602797150611877\n",
      "Epoch 277 loss:  0.7598680257797241\n",
      "Epoch 278 loss:  0.759457528591156\n",
      "Epoch 279 loss:  0.7590470910072327\n",
      "Epoch 280 loss:  0.7586376667022705\n",
      "Epoch 281 loss:  0.75822913646698\n",
      "Epoch 282 loss:  0.7578210234642029\n",
      "Epoch 283 loss:  0.7574137449264526\n",
      "Epoch 284 loss:  0.7570067048072815\n",
      "Epoch 285 loss:  0.756600558757782\n",
      "Epoch 286 loss:  0.7561951875686646\n",
      "Epoch 287 loss:  0.7557901740074158\n",
      "Epoch 288 loss:  0.7553857564926147\n",
      "Epoch 289 loss:  0.7549821138381958\n",
      "Epoch 290 loss:  0.7545793652534485\n",
      "Epoch 291 loss:  0.7541769742965698\n",
      "Epoch 292 loss:  0.7537752389907837\n",
      "Epoch 293 loss:  0.7533737421035767\n",
      "Epoch 294 loss:  0.752973198890686\n",
      "Epoch 295 loss:  0.7525733113288879\n",
      "Epoch 296 loss:  0.7521737217903137\n",
      "Epoch 297 loss:  0.7517749071121216\n",
      "Epoch 298 loss:  0.7513765692710876\n",
      "Epoch 299 loss:  0.7509790062904358\n",
      "Epoch 300 loss:  0.7505816221237183\n",
      "Epoch 301 loss:  0.7501853108406067\n",
      "Epoch 302 loss:  0.7497892379760742\n",
      "Epoch 303 loss:  0.7493943572044373\n",
      "Epoch 304 loss:  0.7489996552467346\n",
      "Epoch 305 loss:  0.7486056685447693\n",
      "Epoch 306 loss:  0.7482120394706726\n",
      "Epoch 307 loss:  0.7478190660476685\n",
      "Epoch 308 loss:  0.7474267482757568\n",
      "Epoch 309 loss:  0.7470354437828064\n",
      "Epoch 310 loss:  0.7466442584991455\n",
      "Epoch 311 loss:  0.7462539076805115\n",
      "Epoch 312 loss:  0.7458641529083252\n",
      "Epoch 313 loss:  0.7454747557640076\n",
      "Epoch 314 loss:  0.745086133480072\n",
      "Epoch 315 loss:  0.7446978688240051\n",
      "Epoch 316 loss:  0.7443104386329651\n",
      "Epoch 317 loss:  0.7439233660697937\n",
      "Epoch 318 loss:  0.7435368299484253\n",
      "Epoch 319 loss:  0.7431512475013733\n",
      "Epoch 320 loss:  0.7427656054496765\n",
      "Epoch 321 loss:  0.7423809170722961\n",
      "Epoch 322 loss:  0.7419965267181396\n",
      "Epoch 323 loss:  0.7416128516197205\n",
      "Epoch 324 loss:  0.7412298321723938\n",
      "Epoch 325 loss:  0.7408473491668701\n",
      "Epoch 326 loss:  0.7404655814170837\n",
      "Epoch 327 loss:  0.7400842905044556\n",
      "Epoch 328 loss:  0.7397041916847229\n",
      "Epoch 329 loss:  0.7393238544464111\n",
      "Epoch 330 loss:  0.7389447093009949\n",
      "Epoch 331 loss:  0.7385660409927368\n",
      "Epoch 332 loss:  0.7381876111030579\n",
      "Epoch 333 loss:  0.7378098368644714\n",
      "Epoch 334 loss:  0.7374326586723328\n",
      "Epoch 335 loss:  0.737056314945221\n",
      "Epoch 336 loss:  0.7366803884506226\n",
      "Epoch 337 loss:  0.7363046407699585\n",
      "Epoch 338 loss:  0.7359298467636108\n",
      "Epoch 339 loss:  0.7355555891990662\n",
      "Epoch 340 loss:  0.7351821660995483\n",
      "Epoch 341 loss:  0.734809160232544\n",
      "Epoch 342 loss:  0.7344368100166321\n",
      "Epoch 343 loss:  0.7340649962425232\n",
      "Epoch 344 loss:  0.733694314956665\n",
      "Epoch 345 loss:  0.7333237528800964\n",
      "Epoch 346 loss:  0.7329539656639099\n",
      "Epoch 347 loss:  0.7325848340988159\n",
      "Epoch 348 loss:  0.7322164177894592\n",
      "Epoch 349 loss:  0.7318481802940369\n",
      "Epoch 350 loss:  0.7314810156822205\n",
      "Epoch 351 loss:  0.7311146259307861\n",
      "Epoch 352 loss:  0.7307485342025757\n",
      "Epoch 353 loss:  0.7303832173347473\n",
      "Epoch 354 loss:  0.7300182580947876\n",
      "Epoch 355 loss:  0.7296540141105652\n",
      "Epoch 356 loss:  0.7292907238006592\n",
      "Epoch 357 loss:  0.7289273142814636\n",
      "Epoch 358 loss:  0.7285649180412292\n",
      "Epoch 359 loss:  0.7282029986381531\n",
      "Epoch 360 loss:  0.7278420329093933\n",
      "Epoch 361 loss:  0.7274813055992126\n",
      "Epoch 362 loss:  0.7271211743354797\n",
      "Epoch 363 loss:  0.7267616987228394\n",
      "Epoch 364 loss:  0.726402997970581\n",
      "Epoch 365 loss:  0.726044774055481\n",
      "Epoch 366 loss:  0.7256869673728943\n",
      "Epoch 367 loss:  0.7253296971321106\n",
      "Epoch 368 loss:  0.7249733805656433\n",
      "Epoch 369 loss:  0.7246177792549133\n",
      "Epoch 370 loss:  0.7242624759674072\n",
      "Epoch 371 loss:  0.7239081263542175\n",
      "Epoch 372 loss:  0.7235538959503174\n",
      "Epoch 373 loss:  0.7232005596160889\n",
      "Epoch 374 loss:  0.7228479981422424\n",
      "Epoch 375 loss:  0.7224956154823303\n",
      "Epoch 376 loss:  0.7221443057060242\n",
      "Epoch 377 loss:  0.7217932939529419\n",
      "Epoch 378 loss:  0.7214431166648865\n",
      "Epoch 379 loss:  0.7210934162139893\n",
      "Epoch 380 loss:  0.7207444310188293\n",
      "Epoch 381 loss:  0.7203961610794067\n",
      "Epoch 382 loss:  0.7200484871864319\n",
      "Epoch 383 loss:  0.71970134973526\n",
      "Epoch 384 loss:  0.7193552255630493\n",
      "Epoch 385 loss:  0.7190091609954834\n",
      "Epoch 386 loss:  0.7186639308929443\n",
      "Epoch 387 loss:  0.7183196544647217\n",
      "Epoch 388 loss:  0.7179754972457886\n",
      "Epoch 389 loss:  0.7176322937011719\n",
      "Epoch 390 loss:  0.7172895669937134\n",
      "Epoch 391 loss:  0.7169475555419922\n",
      "Epoch 392 loss:  0.716606080532074\n",
      "Epoch 393 loss:  0.7162656188011169\n",
      "Epoch 394 loss:  0.715925395488739\n",
      "Epoch 395 loss:  0.7155860066413879\n",
      "Epoch 396 loss:  0.7152471542358398\n",
      "Epoch 397 loss:  0.7149088382720947\n",
      "Epoch 398 loss:  0.7145712971687317\n",
      "Epoch 399 loss:  0.7142344117164612\n",
      "Epoch 400 loss:  0.7138984203338623\n",
      "Epoch 401 loss:  0.7135629653930664\n",
      "Epoch 402 loss:  0.7132283449172974\n",
      "Epoch 403 loss:  0.7128944993019104\n",
      "Epoch 404 loss:  0.7125608325004578\n",
      "Epoch 405 loss:  0.7122281789779663\n",
      "Epoch 406 loss:  0.7118963003158569\n",
      "Epoch 407 loss:  0.7115650773048401\n",
      "Epoch 408 loss:  0.7112347483634949\n",
      "Epoch 409 loss:  0.7109046578407288\n",
      "Epoch 410 loss:  0.7105753421783447\n",
      "Epoch 411 loss:  0.7102466821670532\n",
      "Epoch 412 loss:  0.7099188566207886\n",
      "Epoch 413 loss:  0.7095915675163269\n",
      "Epoch 414 loss:  0.7092651128768921\n",
      "Epoch 415 loss:  0.7089390158653259\n",
      "Epoch 416 loss:  0.7086138129234314\n",
      "Epoch 417 loss:  0.7082896828651428\n",
      "Epoch 418 loss:  0.7079656720161438\n",
      "Epoch 419 loss:  0.7076429128646851\n",
      "Epoch 420 loss:  0.7073209881782532\n",
      "Epoch 421 loss:  0.7069993615150452\n",
      "Epoch 422 loss:  0.7066786289215088\n",
      "Epoch 423 loss:  0.7063585519790649\n",
      "Epoch 424 loss:  0.7060392498970032\n",
      "Epoch 425 loss:  0.7057203650474548\n",
      "Epoch 426 loss:  0.7054024934768677\n",
      "Epoch 427 loss:  0.7050853967666626\n",
      "Epoch 428 loss:  0.7047684192657471\n",
      "Epoch 429 loss:  0.7044527530670166\n",
      "Epoch 430 loss:  0.7041375041007996\n",
      "Epoch 431 loss:  0.7038231492042542\n",
      "Epoch 432 loss:  0.7035092711448669\n",
      "Epoch 433 loss:  0.7031963467597961\n",
      "Epoch 434 loss:  0.7028839588165283\n",
      "Epoch 435 loss:  0.7025721669197083\n",
      "Epoch 436 loss:  0.7022612690925598\n",
      "Epoch 437 loss:  0.7019511461257935\n",
      "Epoch 438 loss:  0.7016414999961853\n",
      "Epoch 439 loss:  0.7013325095176697\n",
      "Epoch 440 loss:  0.7010244727134705\n",
      "Epoch 441 loss:  0.7007166743278503\n",
      "Epoch 442 loss:  0.7004100680351257\n",
      "Epoch 443 loss:  0.700103759765625\n",
      "Epoch 444 loss:  0.6997982263565063\n",
      "Epoch 445 loss:  0.6994935274124146\n",
      "Epoch 446 loss:  0.6991894841194153\n",
      "Epoch 447 loss:  0.6988859176635742\n",
      "Epoch 448 loss:  0.6985833644866943\n",
      "Epoch 449 loss:  0.698281466960907\n",
      "Epoch 450 loss:  0.6979799866676331\n",
      "Epoch 451 loss:  0.6976795792579651\n",
      "Epoch 452 loss:  0.697379469871521\n",
      "Epoch 453 loss:  0.6970800161361694\n",
      "Epoch 454 loss:  0.6967815160751343\n",
      "Epoch 455 loss:  0.6964837312698364\n",
      "Epoch 456 loss:  0.6961866021156311\n",
      "Epoch 457 loss:  0.6958901882171631\n",
      "Epoch 458 loss:  0.6955943703651428\n",
      "Epoch 459 loss:  0.6952992677688599\n",
      "Epoch 460 loss:  0.6950045824050903\n",
      "Epoch 461 loss:  0.6947112679481506\n",
      "Epoch 462 loss:  0.6944178938865662\n",
      "Epoch 463 loss:  0.694125771522522\n",
      "Epoch 464 loss:  0.6938341856002808\n",
      "Epoch 465 loss:  0.6935433745384216\n",
      "Epoch 466 loss:  0.6932532787322998\n",
      "Epoch 467 loss:  0.6929635405540466\n",
      "Epoch 468 loss:  0.6926746964454651\n",
      "Epoch 469 loss:  0.6923867464065552\n",
      "Epoch 470 loss:  0.6920992732048035\n",
      "Epoch 471 loss:  0.6918126344680786\n",
      "Epoch 472 loss:  0.691526472568512\n",
      "Epoch 473 loss:  0.6912412047386169\n",
      "Epoch 474 loss:  0.6909564733505249\n",
      "Epoch 475 loss:  0.6906726956367493\n",
      "Epoch 476 loss:  0.6903893947601318\n",
      "Epoch 477 loss:  0.6901068091392517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 478 loss:  0.6898252367973328\n",
      "Epoch 479 loss:  0.6895438432693481\n",
      "Epoch 480 loss:  0.6892633438110352\n",
      "Epoch 481 loss:  0.6889838576316833\n",
      "Epoch 482 loss:  0.6887046694755554\n",
      "Epoch 483 loss:  0.6884262561798096\n",
      "Epoch 484 loss:  0.6881487965583801\n",
      "Epoch 485 loss:  0.687872052192688\n",
      "Epoch 486 loss:  0.6875959634780884\n",
      "Epoch 487 loss:  0.6873204112052917\n",
      "Epoch 488 loss:  0.6870455145835876\n",
      "Epoch 489 loss:  0.6867715120315552\n",
      "Epoch 490 loss:  0.6864978671073914\n",
      "Epoch 491 loss:  0.6862255334854126\n",
      "Epoch 492 loss:  0.685953676700592\n",
      "Epoch 493 loss:  0.6856825351715088\n",
      "Epoch 494 loss:  0.6854120492935181\n",
      "Epoch 495 loss:  0.6851426959037781\n",
      "Epoch 496 loss:  0.6848734021186829\n",
      "Epoch 497 loss:  0.6846051812171936\n",
      "Epoch 498 loss:  0.684337854385376\n",
      "Epoch 499 loss:  0.6840712428092957\n",
      "Epoch 500 loss:  0.6838054060935974\n",
      "Epoch 501 loss:  0.6835401058197021\n",
      "Epoch 502 loss:  0.6832753419876099\n",
      "Epoch 503 loss:  0.6830112338066101\n",
      "Epoch 504 loss:  0.682748556137085\n",
      "Epoch 505 loss:  0.682485818862915\n",
      "Epoch 506 loss:  0.682223916053772\n",
      "Epoch 507 loss:  0.681962788105011\n",
      "Epoch 508 loss:  0.6817020773887634\n",
      "Epoch 509 loss:  0.6814426183700562\n",
      "Epoch 510 loss:  0.6811832785606384\n",
      "Epoch 511 loss:  0.6809251308441162\n",
      "Epoch 512 loss:  0.6806674599647522\n",
      "Epoch 513 loss:  0.6804102659225464\n",
      "Epoch 514 loss:  0.6801540851593018\n",
      "Epoch 515 loss:  0.6798981428146362\n",
      "Epoch 516 loss:  0.6796433329582214\n",
      "Epoch 517 loss:  0.6793889999389648\n",
      "Epoch 518 loss:  0.6791355013847351\n",
      "Epoch 519 loss:  0.6788824200630188\n",
      "Epoch 520 loss:  0.6786300539970398\n",
      "Epoch 521 loss:  0.6783784627914429\n",
      "Epoch 522 loss:  0.6781272292137146\n",
      "Epoch 523 loss:  0.6778773069381714\n",
      "Epoch 524 loss:  0.6776275634765625\n",
      "Epoch 525 loss:  0.6773786544799805\n",
      "Epoch 526 loss:  0.6771304607391357\n",
      "Epoch 527 loss:  0.6768827438354492\n",
      "Epoch 528 loss:  0.6766357421875\n",
      "Epoch 529 loss:  0.6763896346092224\n",
      "Epoch 530 loss:  0.6761440634727478\n",
      "Epoch 531 loss:  0.675899088382721\n",
      "Epoch 532 loss:  0.6756548285484314\n",
      "Epoch 533 loss:  0.6754112243652344\n",
      "Epoch 534 loss:  0.6751681566238403\n",
      "Epoch 535 loss:  0.6749259829521179\n",
      "Epoch 536 loss:  0.6746845841407776\n",
      "Epoch 537 loss:  0.6744434833526611\n",
      "Epoch 538 loss:  0.6742034554481506\n",
      "Epoch 539 loss:  0.6739634275436401\n",
      "Epoch 540 loss:  0.6737247109413147\n",
      "Epoch 541 loss:  0.6734864711761475\n",
      "Epoch 542 loss:  0.673248827457428\n",
      "Epoch 543 loss:  0.6730117797851562\n",
      "Epoch 544 loss:  0.6727754473686218\n",
      "Epoch 545 loss:  0.6725397109985352\n",
      "Epoch 546 loss:  0.6723045706748962\n",
      "Epoch 547 loss:  0.6720703840255737\n",
      "Epoch 548 loss:  0.6718367338180542\n",
      "Epoch 549 loss:  0.6716035008430481\n",
      "Epoch 550 loss:  0.6713711619377136\n",
      "Epoch 551 loss:  0.6711394190788269\n",
      "Epoch 552 loss:  0.6709083914756775\n",
      "Epoch 553 loss:  0.6706777215003967\n",
      "Epoch 554 loss:  0.6704477667808533\n",
      "Epoch 555 loss:  0.6702191233634949\n",
      "Epoch 556 loss:  0.6699904799461365\n",
      "Epoch 557 loss:  0.6697626113891602\n",
      "Epoch 558 loss:  0.6695356369018555\n",
      "Epoch 559 loss:  0.6693090796470642\n",
      "Epoch 560 loss:  0.6690834164619446\n",
      "Epoch 561 loss:  0.6688582301139832\n",
      "Epoch 562 loss:  0.6686335802078247\n",
      "Epoch 563 loss:  0.668409526348114\n",
      "Epoch 564 loss:  0.6681864857673645\n",
      "Epoch 565 loss:  0.6679636836051941\n",
      "Epoch 566 loss:  0.667741596698761\n",
      "Epoch 567 loss:  0.6675199866294861\n",
      "Epoch 568 loss:  0.6672994494438171\n",
      "Epoch 569 loss:  0.6670792102813721\n",
      "Epoch 570 loss:  0.6668598055839539\n",
      "Epoch 571 loss:  0.6666408181190491\n",
      "Epoch 572 loss:  0.666422426700592\n",
      "Epoch 573 loss:  0.6662046313285828\n",
      "Epoch 574 loss:  0.6659874320030212\n",
      "Epoch 575 loss:  0.6657706499099731\n",
      "Epoch 576 loss:  0.665554940700531\n",
      "Epoch 577 loss:  0.6653393507003784\n",
      "Epoch 578 loss:  0.665124773979187\n",
      "Epoch 579 loss:  0.664910614490509\n",
      "Epoch 580 loss:  0.6646969318389893\n",
      "Epoch 581 loss:  0.6644840240478516\n",
      "Epoch 582 loss:  0.6642719507217407\n",
      "Epoch 583 loss:  0.6640602350234985\n",
      "Epoch 584 loss:  0.6638491749763489\n",
      "Epoch 585 loss:  0.6636385321617126\n",
      "Epoch 586 loss:  0.6634289622306824\n",
      "Epoch 587 loss:  0.6632194519042969\n",
      "Epoch 588 loss:  0.6630105972290039\n",
      "Epoch 589 loss:  0.6628021597862244\n",
      "Epoch 590 loss:  0.6625946760177612\n",
      "Epoch 591 loss:  0.6623877286911011\n",
      "Epoch 592 loss:  0.6621811389923096\n",
      "Epoch 593 loss:  0.6619754433631897\n",
      "Epoch 594 loss:  0.6617699861526489\n",
      "Epoch 595 loss:  0.6615654230117798\n",
      "Epoch 596 loss:  0.661361575126648\n",
      "Epoch 597 loss:  0.66115802526474\n",
      "Epoch 598 loss:  0.6609550714492798\n",
      "Epoch 599 loss:  0.6607525944709778\n",
      "Epoch 600 loss:  0.6605511903762817\n",
      "Epoch 601 loss:  0.6603498458862305\n",
      "Epoch 602 loss:  0.6601491570472717\n",
      "Epoch 603 loss:  0.6599485278129578\n",
      "Epoch 604 loss:  0.6597493886947632\n",
      "Epoch 605 loss:  0.6595504283905029\n",
      "Epoch 606 loss:  0.6593523621559143\n",
      "Epoch 607 loss:  0.6591542959213257\n",
      "Epoch 608 loss:  0.6589573621749878\n",
      "Epoch 609 loss:  0.6587604880332947\n",
      "Epoch 610 loss:  0.6585645079612732\n",
      "Epoch 611 loss:  0.6583690643310547\n",
      "Epoch 612 loss:  0.6581741571426392\n",
      "Epoch 613 loss:  0.6579796075820923\n",
      "Epoch 614 loss:  0.6577860713005066\n",
      "Epoch 615 loss:  0.6575926542282104\n",
      "Epoch 616 loss:  0.6574000716209412\n",
      "Epoch 617 loss:  0.6572080254554749\n",
      "Epoch 618 loss:  0.6570163369178772\n",
      "Epoch 619 loss:  0.656825065612793\n",
      "Epoch 620 loss:  0.6566348671913147\n",
      "Epoch 621 loss:  0.6564449667930603\n",
      "Epoch 622 loss:  0.6562560200691223\n",
      "Epoch 623 loss:  0.6560671925544739\n",
      "Epoch 624 loss:  0.6558792591094971\n",
      "Epoch 625 loss:  0.6556916832923889\n",
      "Epoch 626 loss:  0.6555046439170837\n",
      "Epoch 627 loss:  0.655318558216095\n",
      "Epoch 628 loss:  0.655132532119751\n",
      "Epoch 629 loss:  0.6549472212791443\n",
      "Epoch 630 loss:  0.6547624468803406\n",
      "Epoch 631 loss:  0.6545780897140503\n",
      "Epoch 632 loss:  0.6543944478034973\n",
      "Epoch 633 loss:  0.6542112231254578\n",
      "Epoch 634 loss:  0.6540287733078003\n",
      "Epoch 635 loss:  0.6538466215133667\n",
      "Epoch 636 loss:  0.6536651253700256\n",
      "Epoch 637 loss:  0.6534842848777771\n",
      "Epoch 638 loss:  0.6533044576644897\n",
      "Epoch 639 loss:  0.6531246900558472\n",
      "Epoch 640 loss:  0.6529456377029419\n",
      "Epoch 641 loss:  0.6527674198150635\n",
      "Epoch 642 loss:  0.6525890827178955\n",
      "Epoch 643 loss:  0.6524117588996887\n",
      "Epoch 644 loss:  0.6522346138954163\n",
      "Epoch 645 loss:  0.652057945728302\n",
      "Epoch 646 loss:  0.6518818140029907\n",
      "Epoch 647 loss:  0.6517063975334167\n",
      "Epoch 648 loss:  0.6515311002731323\n",
      "Epoch 649 loss:  0.6513566374778748\n",
      "Epoch 650 loss:  0.6511827707290649\n",
      "Epoch 651 loss:  0.6510095000267029\n",
      "Epoch 652 loss:  0.6508368849754333\n",
      "Epoch 653 loss:  0.6506645679473877\n",
      "Epoch 654 loss:  0.6504929661750793\n",
      "Epoch 655 loss:  0.6503218412399292\n",
      "Epoch 656 loss:  0.6501508355140686\n",
      "Epoch 657 loss:  0.6499805450439453\n",
      "Epoch 658 loss:  0.6498109698295593\n",
      "Epoch 659 loss:  0.6496418118476868\n",
      "Epoch 660 loss:  0.6494730710983276\n",
      "Epoch 661 loss:  0.6493048071861267\n",
      "Epoch 662 loss:  0.6491373777389526\n",
      "Epoch 663 loss:  0.6489700078964233\n",
      "Epoch 664 loss:  0.6488033533096313\n",
      "Epoch 665 loss:  0.6486369371414185\n",
      "Epoch 666 loss:  0.6484712958335876\n",
      "Epoch 667 loss:  0.6483059525489807\n",
      "Epoch 668 loss:  0.6481414437294006\n",
      "Epoch 669 loss:  0.6479769349098206\n",
      "Epoch 670 loss:  0.647813081741333\n",
      "Epoch 671 loss:  0.647649884223938\n",
      "Epoch 672 loss:  0.6474871039390564\n",
      "Epoch 673 loss:  0.6473247408866882\n",
      "Epoch 674 loss:  0.6471627950668335\n",
      "Epoch 675 loss:  0.647001326084137\n",
      "Epoch 676 loss:  0.6468404531478882\n",
      "Epoch 677 loss:  0.6466799974441528\n",
      "Epoch 678 loss:  0.6465200781822205\n",
      "Epoch 679 loss:  0.6463608145713806\n",
      "Epoch 680 loss:  0.6462018489837646\n",
      "Epoch 681 loss:  0.6460438966751099\n",
      "Epoch 682 loss:  0.6458861827850342\n",
      "Epoch 683 loss:  0.6457288861274719\n",
      "Epoch 684 loss:  0.6455720067024231\n",
      "Epoch 685 loss:  0.6454156637191772\n",
      "Epoch 686 loss:  0.6452597975730896\n",
      "Epoch 687 loss:  0.6451045274734497\n",
      "Epoch 688 loss:  0.6449497938156128\n",
      "Epoch 689 loss:  0.6447955965995789\n",
      "Epoch 690 loss:  0.6446416974067688\n",
      "Epoch 691 loss:  0.6444879770278931\n",
      "Epoch 692 loss:  0.6443348526954651\n",
      "Epoch 693 loss:  0.6441825032234192\n",
      "Epoch 694 loss:  0.6440304517745972\n",
      "Epoch 695 loss:  0.6438788771629333\n",
      "Epoch 696 loss:  0.6437276601791382\n",
      "Epoch 697 loss:  0.6435768604278564\n",
      "Epoch 698 loss:  0.6434262990951538\n",
      "Epoch 699 loss:  0.643276035785675\n",
      "Epoch 700 loss:  0.643126368522644\n",
      "Epoch 701 loss:  0.6429770588874817\n",
      "Epoch 702 loss:  0.6428282260894775\n",
      "Epoch 703 loss:  0.6426798701286316\n",
      "Epoch 704 loss:  0.6425319910049438\n",
      "Epoch 705 loss:  0.64238440990448\n",
      "Epoch 706 loss:  0.6422373056411743\n",
      "Epoch 707 loss:  0.6420907378196716\n",
      "Epoch 708 loss:  0.6419444680213928\n",
      "Epoch 709 loss:  0.641798734664917\n",
      "Epoch 710 loss:  0.6416535973548889\n",
      "Epoch 711 loss:  0.6415091156959534\n",
      "Epoch 712 loss:  0.6413649916648865\n",
      "Epoch 713 loss:  0.6412208676338196\n",
      "Epoch 714 loss:  0.6410778760910034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 715 loss:  0.6409346461296082\n",
      "Epoch 716 loss:  0.6407923102378845\n",
      "Epoch 717 loss:  0.6406499743461609\n",
      "Epoch 718 loss:  0.6405085325241089\n",
      "Epoch 719 loss:  0.6403675079345703\n",
      "Epoch 720 loss:  0.6402269005775452\n",
      "Epoch 721 loss:  0.6400865316390991\n",
      "Epoch 722 loss:  0.6399468779563904\n",
      "Epoch 723 loss:  0.639807403087616\n",
      "Epoch 724 loss:  0.6396684050559998\n",
      "Epoch 725 loss:  0.6395298838615417\n",
      "Epoch 726 loss:  0.6393913626670837\n",
      "Epoch 727 loss:  0.6392536163330078\n",
      "Epoch 728 loss:  0.6391163468360901\n",
      "Epoch 729 loss:  0.6389795541763306\n",
      "Epoch 730 loss:  0.6388428807258606\n",
      "Epoch 731 loss:  0.638706624507904\n",
      "Epoch 732 loss:  0.6385711431503296\n",
      "Epoch 733 loss:  0.6384355425834656\n",
      "Epoch 734 loss:  0.638300895690918\n",
      "Epoch 735 loss:  0.6381660103797913\n",
      "Epoch 736 loss:  0.6380319595336914\n",
      "Epoch 737 loss:  0.637898325920105\n",
      "Epoch 738 loss:  0.6377649307250977\n",
      "Epoch 739 loss:  0.6376323699951172\n",
      "Epoch 740 loss:  0.6374997496604919\n",
      "Epoch 741 loss:  0.637367844581604\n",
      "Epoch 742 loss:  0.6372361779212952\n",
      "Epoch 743 loss:  0.6371049284934998\n",
      "Epoch 744 loss:  0.6369740962982178\n",
      "Epoch 745 loss:  0.6368435025215149\n",
      "Epoch 746 loss:  0.636713445186615\n",
      "Epoch 747 loss:  0.6365839242935181\n",
      "Epoch 748 loss:  0.6364546418190002\n",
      "Epoch 749 loss:  0.6363258957862854\n",
      "Epoch 750 loss:  0.6361976265907288\n",
      "Epoch 751 loss:  0.6360698938369751\n",
      "Epoch 752 loss:  0.6359425783157349\n",
      "Epoch 753 loss:  0.6358155608177185\n",
      "Epoch 754 loss:  0.6356889009475708\n",
      "Epoch 755 loss:  0.6355628967285156\n",
      "Epoch 756 loss:  0.6354369521141052\n",
      "Epoch 757 loss:  0.6353116631507874\n",
      "Epoch 758 loss:  0.6351862549781799\n",
      "Epoch 759 loss:  0.6350619196891785\n",
      "Epoch 760 loss:  0.6349376440048218\n",
      "Epoch 761 loss:  0.6348137259483337\n",
      "Epoch 762 loss:  0.6346901655197144\n",
      "Epoch 763 loss:  0.6345667243003845\n",
      "Epoch 764 loss:  0.634443998336792\n",
      "Epoch 765 loss:  0.6343216300010681\n",
      "Epoch 766 loss:  0.6341997385025024\n",
      "Epoch 767 loss:  0.6340778470039368\n",
      "Epoch 768 loss:  0.6339566111564636\n",
      "Epoch 769 loss:  0.6338358521461487\n",
      "Epoch 770 loss:  0.633715033531189\n",
      "Epoch 771 loss:  0.6335946321487427\n",
      "Epoch 772 loss:  0.6334749460220337\n",
      "Epoch 773 loss:  0.6333555579185486\n",
      "Epoch 774 loss:  0.633236289024353\n",
      "Epoch 775 loss:  0.6331174969673157\n",
      "Epoch 776 loss:  0.6329988241195679\n",
      "Epoch 777 loss:  0.6328809261322021\n",
      "Epoch 778 loss:  0.6327630281448364\n",
      "Epoch 779 loss:  0.6326457858085632\n",
      "Epoch 780 loss:  0.63252854347229\n",
      "Epoch 781 loss:  0.6324115991592407\n",
      "Epoch 782 loss:  0.632295548915863\n",
      "Epoch 783 loss:  0.6321794986724854\n",
      "Epoch 784 loss:  0.6320636868476868\n",
      "Epoch 785 loss:  0.6319484114646912\n",
      "Epoch 786 loss:  0.6318332552909851\n",
      "Epoch 787 loss:  0.6317186951637268\n",
      "Epoch 788 loss:  0.6316041946411133\n",
      "Epoch 789 loss:  0.6314903497695923\n",
      "Epoch 790 loss:  0.6313764452934265\n",
      "Epoch 791 loss:  0.6312633156776428\n",
      "Epoch 792 loss:  0.6311501860618591\n",
      "Epoch 793 loss:  0.6310376524925232\n",
      "Epoch 794 loss:  0.6309249401092529\n",
      "Epoch 795 loss:  0.6308133602142334\n",
      "Epoch 796 loss:  0.63070148229599\n",
      "Epoch 797 loss:  0.6305901408195496\n",
      "Epoch 798 loss:  0.6304792165756226\n",
      "Epoch 799 loss:  0.6303686499595642\n",
      "Epoch 800 loss:  0.630258321762085\n",
      "Epoch 801 loss:  0.6301482319831848\n",
      "Epoch 802 loss:  0.6300386786460876\n",
      "Epoch 803 loss:  0.6299296021461487\n",
      "Epoch 804 loss:  0.6298204064369202\n",
      "Epoch 805 loss:  0.6297118067741394\n",
      "Epoch 806 loss:  0.6296031475067139\n",
      "Epoch 807 loss:  0.6294952630996704\n",
      "Epoch 808 loss:  0.6293874382972717\n",
      "Epoch 809 loss:  0.6292803883552551\n",
      "Epoch 810 loss:  0.6291728615760803\n",
      "Epoch 811 loss:  0.6290662288665771\n",
      "Epoch 812 loss:  0.628959596157074\n",
      "Epoch 813 loss:  0.628853440284729\n",
      "Epoch 814 loss:  0.628747820854187\n",
      "Epoch 815 loss:  0.6286421418190002\n",
      "Epoch 816 loss:  0.6285368204116821\n",
      "Epoch 817 loss:  0.628432035446167\n",
      "Epoch 818 loss:  0.6283274292945862\n",
      "Epoch 819 loss:  0.6282228827476501\n",
      "Epoch 820 loss:  0.6281187534332275\n",
      "Epoch 821 loss:  0.628015398979187\n",
      "Epoch 822 loss:  0.6279118657112122\n",
      "Epoch 823 loss:  0.6278086304664612\n",
      "Epoch 824 loss:  0.6277058720588684\n",
      "Epoch 825 loss:  0.6276029944419861\n",
      "Epoch 826 loss:  0.6275008916854858\n",
      "Epoch 827 loss:  0.6273990869522095\n",
      "Epoch 828 loss:  0.6272973418235779\n",
      "Epoch 829 loss:  0.6271957755088806\n",
      "Epoch 830 loss:  0.6270948052406311\n",
      "Epoch 831 loss:  0.6269940137863159\n",
      "Epoch 832 loss:  0.6268931031227112\n",
      "Epoch 833 loss:  0.6267930865287781\n",
      "Epoch 834 loss:  0.6266931891441345\n",
      "Epoch 835 loss:  0.6265933513641357\n",
      "Epoch 836 loss:  0.6264941096305847\n",
      "Epoch 837 loss:  0.6263948678970337\n",
      "Epoch 838 loss:  0.6262959837913513\n",
      "Epoch 839 loss:  0.6261975765228271\n",
      "Epoch 840 loss:  0.6260994076728821\n",
      "Epoch 841 loss:  0.626001238822937\n",
      "Epoch 842 loss:  0.6259034276008606\n",
      "Epoch 843 loss:  0.6258060336112976\n",
      "Epoch 844 loss:  0.6257086992263794\n",
      "Epoch 845 loss:  0.6256122589111328\n",
      "Epoch 846 loss:  0.6255151033401489\n",
      "Epoch 847 loss:  0.6254188418388367\n",
      "Epoch 848 loss:  0.6253227591514587\n",
      "Epoch 849 loss:  0.625227153301239\n",
      "Epoch 850 loss:  0.6251316070556641\n",
      "Epoch 851 loss:  0.6250364184379578\n",
      "Epoch 852 loss:  0.6249412894248962\n",
      "Epoch 853 loss:  0.6248468160629272\n",
      "Epoch 854 loss:  0.6247524619102478\n",
      "Epoch 855 loss:  0.6246580481529236\n",
      "Epoch 856 loss:  0.6245642900466919\n",
      "Epoch 857 loss:  0.6244710087776184\n",
      "Epoch 858 loss:  0.6243776082992554\n",
      "Epoch 859 loss:  0.6242843866348267\n",
      "Epoch 860 loss:  0.6241917014122009\n",
      "Epoch 861 loss:  0.6240991353988647\n",
      "Epoch 862 loss:  0.6240066885948181\n",
      "Epoch 863 loss:  0.6239145994186401\n",
      "Epoch 864 loss:  0.6238229870796204\n",
      "Epoch 865 loss:  0.6237313747406006\n",
      "Epoch 866 loss:  0.6236401796340942\n",
      "Epoch 867 loss:  0.6235492825508118\n",
      "Epoch 868 loss:  0.6234586238861084\n",
      "Epoch 869 loss:  0.6233678460121155\n",
      "Epoch 870 loss:  0.6232777237892151\n",
      "Epoch 871 loss:  0.6231876015663147\n",
      "Epoch 872 loss:  0.623097836971283\n",
      "Epoch 873 loss:  0.6230084896087646\n",
      "Epoch 874 loss:  0.6229192614555359\n",
      "Epoch 875 loss:  0.6228300333023071\n",
      "Epoch 876 loss:  0.622741162776947\n",
      "Epoch 877 loss:  0.6226527690887451\n",
      "Epoch 878 loss:  0.622564435005188\n",
      "Epoch 879 loss:  0.6224763989448547\n",
      "Epoch 880 loss:  0.6223885416984558\n",
      "Epoch 881 loss:  0.6223011016845703\n",
      "Epoch 882 loss:  0.6222139000892639\n",
      "Epoch 883 loss:  0.6221268177032471\n",
      "Epoch 884 loss:  0.6220399141311646\n",
      "Epoch 885 loss:  0.6219531297683716\n",
      "Epoch 886 loss:  0.6218670010566711\n",
      "Epoch 887 loss:  0.6217808127403259\n",
      "Epoch 888 loss:  0.6216951012611389\n",
      "Epoch 889 loss:  0.6216091513633728\n",
      "Epoch 890 loss:  0.6215238571166992\n",
      "Epoch 891 loss:  0.6214385628700256\n",
      "Epoch 892 loss:  0.6213536858558655\n",
      "Epoch 893 loss:  0.6212689280509949\n",
      "Epoch 894 loss:  0.6211844682693481\n",
      "Epoch 895 loss:  0.6211001873016357\n",
      "Epoch 896 loss:  0.6210162043571472\n",
      "Epoch 897 loss:  0.6209322810173035\n",
      "Epoch 898 loss:  0.6208487153053284\n",
      "Epoch 899 loss:  0.6207653284072876\n",
      "Epoch 900 loss:  0.6206823587417603\n",
      "Epoch 901 loss:  0.6205993890762329\n",
      "Epoch 902 loss:  0.6205164790153503\n",
      "Epoch 903 loss:  0.6204342246055603\n",
      "Epoch 904 loss:  0.6203519701957703\n",
      "Epoch 905 loss:  0.6202700734138489\n",
      "Epoch 906 loss:  0.620188295841217\n",
      "Epoch 907 loss:  0.6201065182685852\n",
      "Epoch 908 loss:  0.6200254559516907\n",
      "Epoch 909 loss:  0.6199444532394409\n",
      "Epoch 910 loss:  0.6198634505271912\n",
      "Epoch 911 loss:  0.619782567024231\n",
      "Epoch 912 loss:  0.6197020411491394\n",
      "Epoch 913 loss:  0.619621992111206\n",
      "Epoch 914 loss:  0.6195420026779175\n",
      "Epoch 915 loss:  0.6194621324539185\n",
      "Epoch 916 loss:  0.6193826794624329\n",
      "Epoch 917 loss:  0.6193031668663025\n",
      "Epoch 918 loss:  0.6192238330841064\n",
      "Epoch 919 loss:  0.6191450357437134\n",
      "Epoch 920 loss:  0.6190661191940308\n",
      "Epoch 921 loss:  0.6189877986907959\n",
      "Epoch 922 loss:  0.6189091205596924\n",
      "Epoch 923 loss:  0.6188309788703918\n",
      "Epoch 924 loss:  0.6187530755996704\n",
      "Epoch 925 loss:  0.6186753511428833\n",
      "Epoch 926 loss:  0.6185979247093201\n",
      "Epoch 927 loss:  0.6185204982757568\n",
      "Epoch 928 loss:  0.6184431910514832\n",
      "Epoch 929 loss:  0.6183663010597229\n",
      "Epoch 930 loss:  0.618289589881897\n",
      "Epoch 931 loss:  0.6182129979133606\n",
      "Epoch 932 loss:  0.6181365847587585\n",
      "Epoch 933 loss:  0.6180602312088013\n",
      "Epoch 934 loss:  0.6179845333099365\n",
      "Epoch 935 loss:  0.6179086565971375\n",
      "Epoch 936 loss:  0.617833137512207\n",
      "Epoch 937 loss:  0.6177577972412109\n",
      "Epoch 938 loss:  0.617682695388794\n",
      "Epoch 939 loss:  0.6176075339317322\n",
      "Epoch 940 loss:  0.6175330281257629\n",
      "Epoch 941 loss:  0.6174583435058594\n",
      "Epoch 942 loss:  0.6173840165138245\n",
      "Epoch 943 loss:  0.6173098683357239\n",
      "Epoch 944 loss:  0.6172359585762024\n",
      "Epoch 945 loss:  0.6171621084213257\n",
      "Epoch 946 loss:  0.6170886158943176\n",
      "Epoch 947 loss:  0.6170150637626648\n",
      "Epoch 948 loss:  0.6169421076774597\n",
      "Epoch 949 loss:  0.6168690919876099\n",
      "Epoch 950 loss:  0.6167961955070496\n",
      "Epoch 951 loss:  0.6167234182357788\n",
      "Epoch 952 loss:  0.6166512966156006\n",
      "Epoch 953 loss:  0.6165790557861328\n",
      "Epoch 954 loss:  0.6165071725845337\n",
      "Epoch 955 loss:  0.6164349913597107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 956 loss:  0.6163633465766907\n",
      "Epoch 957 loss:  0.6162920594215393\n",
      "Epoch 958 loss:  0.6162206530570984\n",
      "Epoch 959 loss:  0.6161495447158813\n",
      "Epoch 960 loss:  0.6160786747932434\n",
      "Epoch 961 loss:  0.6160078644752502\n",
      "Epoch 962 loss:  0.6159374117851257\n",
      "Epoch 963 loss:  0.6158668994903564\n",
      "Epoch 964 loss:  0.6157967448234558\n",
      "Epoch 965 loss:  0.615726888179779\n",
      "Epoch 966 loss:  0.6156567335128784\n",
      "Epoch 967 loss:  0.6155874133110046\n",
      "Epoch 968 loss:  0.615517795085907\n",
      "Epoch 969 loss:  0.615448534488678\n",
      "Epoch 970 loss:  0.6153793334960938\n",
      "Epoch 971 loss:  0.6153104305267334\n",
      "Epoch 972 loss:  0.6152415871620178\n",
      "Epoch 973 loss:  0.6151731014251709\n",
      "Epoch 974 loss:  0.6151047348976135\n",
      "Epoch 975 loss:  0.6150362491607666\n",
      "Epoch 976 loss:  0.6149682402610779\n",
      "Epoch 977 loss:  0.6149001717567444\n",
      "Epoch 978 loss:  0.6148325800895691\n",
      "Epoch 979 loss:  0.6147652864456177\n",
      "Epoch 980 loss:  0.6146977543830872\n",
      "Epoch 981 loss:  0.6146305799484253\n",
      "Epoch 982 loss:  0.6145636439323425\n",
      "Epoch 983 loss:  0.6144969463348389\n",
      "Epoch 984 loss:  0.6144300699234009\n",
      "Epoch 985 loss:  0.6143636703491211\n",
      "Epoch 986 loss:  0.6142972111701965\n",
      "Epoch 987 loss:  0.6142313480377197\n",
      "Epoch 988 loss:  0.6141650676727295\n",
      "Epoch 989 loss:  0.6140992045402527\n",
      "Epoch 990 loss:  0.6140336990356445\n",
      "Epoch 991 loss:  0.6139681339263916\n",
      "Epoch 992 loss:  0.613902747631073\n",
      "Epoch 993 loss:  0.6138375401496887\n",
      "Epoch 994 loss:  0.6137726306915283\n",
      "Epoch 995 loss:  0.6137076616287231\n",
      "Epoch 996 loss:  0.6136432886123657\n",
      "Epoch 997 loss:  0.6135786771774292\n",
      "Epoch 998 loss:  0.6135144829750061\n",
      "Epoch 999 loss:  0.6134501099586487\n",
      "Epoch 1000 loss:  0.6133860945701599\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "running_loss = 0.0\n",
    "\n",
    "#inputs = Variable(torch.tensor(x_train).float())\n",
    "#labels = Variable(torch.tensor(y_train).float())\n",
    "\n",
    "batch_no\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i in range(batch_no):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs = Variable(torch.tensor(x_train[start:end]).float())\n",
    "        labels = Variable(torch.tensor(y_train[start:end]).long())\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print(\"outputs\",outputs)\n",
    "        #print(\"outputs\",outputs,outputs.shape,\"labels\",labels, labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print('Epoch {}'.format(epoch+1), \"loss: \",running_loss)\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.7551e-04, 3.9394e-01, 6.0558e-01],\n",
       "        [4.1037e-05, 1.1331e-01, 8.8665e-01],\n",
       "        [2.2359e-06, 1.4453e-02, 9.8554e-01],\n",
       "        [1.3787e-02, 9.0762e-01, 7.8589e-02],\n",
       "        [9.8614e-01, 1.3856e-02, 8.2584e-09],\n",
       "        [6.2317e-04, 3.4587e-01, 6.5351e-01],\n",
       "        [3.4013e-03, 8.3522e-01, 1.6138e-01],\n",
       "        [9.8894e-01, 1.1056e-02, 1.2191e-08],\n",
       "        [9.7691e-01, 2.3090e-02, 7.1753e-08],\n",
       "        [2.4939e-03, 6.0676e-01, 3.9074e-01],\n",
       "        [3.4333e-04, 3.4840e-01, 6.5126e-01],\n",
       "        [9.9834e-01, 1.6561e-03, 2.0634e-11],\n",
       "        [8.8133e-03, 7.6289e-01, 2.2830e-01],\n",
       "        [5.0453e-05, 1.1983e-01, 8.8011e-01],\n",
       "        [8.2221e-06, 7.3265e-02, 9.2673e-01],\n",
       "        [1.7394e-06, 1.3420e-02, 9.8658e-01],\n",
       "        [9.9654e-01, 3.4642e-03, 1.2719e-10],\n",
       "        [9.9129e-01, 8.7055e-03, 7.3405e-09],\n",
       "        [2.3834e-03, 7.2037e-01, 2.7724e-01],\n",
       "        [9.8332e-01, 1.6681e-02, 1.8906e-08],\n",
       "        [9.9391e-01, 6.0876e-03, 7.8572e-10],\n",
       "        [3.9529e-04, 3.6465e-01, 6.3495e-01],\n",
       "        [9.6805e-01, 3.1947e-02, 8.0456e-08],\n",
       "        [4.7032e-04, 3.1936e-01, 6.8017e-01],\n",
       "        [9.9387e-01, 6.1317e-03, 9.2948e-10],\n",
       "        [9.8816e-01, 1.1842e-02, 2.7701e-09],\n",
       "        [9.9136e-01, 8.6413e-03, 1.6090e-09],\n",
       "        [2.9194e-05, 4.3279e-02, 9.5669e-01],\n",
       "        [4.4332e-06, 5.5603e-02, 9.4439e-01],\n",
       "        [9.9476e-01, 5.2392e-03, 7.5363e-10],\n",
       "        [7.6278e-06, 6.7979e-02, 9.3201e-01],\n",
       "        [5.1137e-05, 9.4939e-02, 9.0501e-01],\n",
       "        [9.7962e-01, 2.0382e-02, 3.2333e-08],\n",
       "        [9.9438e-01, 5.6202e-03, 7.5983e-10],\n",
       "        [1.1340e-02, 9.4355e-01, 4.5107e-02],\n",
       "        [3.9740e-02, 9.2508e-01, 3.5180e-02],\n",
       "        [8.5389e-05, 3.6133e-01, 6.3859e-01],\n",
       "        [9.9303e-01, 6.9742e-03, 9.5435e-10],\n",
       "        [9.9749e-01, 2.5112e-03, 8.2274e-11],\n",
       "        [1.9675e-02, 9.4811e-01, 3.2210e-02],\n",
       "        [1.3993e-02, 8.5235e-01, 1.3366e-01],\n",
       "        [9.9424e-01, 5.7638e-03, 9.6989e-10],\n",
       "        [3.2114e-06, 2.3076e-02, 9.7692e-01],\n",
       "        [8.5479e-05, 1.4710e-01, 8.5282e-01],\n",
       "        [1.6763e-05, 3.6981e-02, 9.6300e-01],\n",
       "        [3.8385e-06, 1.2276e-02, 9.8772e-01],\n",
       "        [5.8344e-06, 2.2404e-02, 9.7759e-01],\n",
       "        [4.9624e-03, 7.3595e-01, 2.5909e-01],\n",
       "        [9.9470e-01, 5.2981e-03, 7.2682e-10],\n",
       "        [9.8500e-01, 1.5005e-02, 2.5322e-08],\n",
       "        [4.2759e-04, 3.5425e-01, 6.4532e-01],\n",
       "        [9.7527e-01, 2.4731e-02, 3.7441e-08],\n",
       "        [9.8614e-01, 1.3856e-02, 8.2584e-09],\n",
       "        [5.1707e-03, 9.3203e-01, 6.2799e-02],\n",
       "        [3.9751e-03, 8.2239e-01, 1.7363e-01],\n",
       "        [7.0593e-03, 8.0780e-01, 1.8515e-01],\n",
       "        [6.1311e-02, 9.3255e-01, 6.1437e-03],\n",
       "        [1.6763e-05, 3.6981e-02, 9.6300e-01],\n",
       "        [1.3805e-02, 9.7080e-01, 1.5395e-02],\n",
       "        [3.7908e-05, 7.5859e-02, 9.2410e-01],\n",
       "        [9.5438e-01, 4.5617e-02, 3.6150e-07],\n",
       "        [2.4346e-05, 7.9488e-02, 9.2049e-01],\n",
       "        [1.2737e-02, 9.1993e-01, 6.7331e-02],\n",
       "        [9.9317e-01, 6.8339e-03, 1.4872e-09],\n",
       "        [9.8607e-01, 1.3928e-02, 1.0898e-08],\n",
       "        [1.0881e-05, 4.2236e-02, 9.5775e-01],\n",
       "        [1.2030e-02, 9.6129e-01, 2.6679e-02],\n",
       "        [3.7830e-07, 1.2944e-02, 9.8706e-01],\n",
       "        [3.4924e-08, 2.6519e-03, 9.9735e-01],\n",
       "        [9.8226e-01, 1.7738e-02, 1.4810e-08],\n",
       "        [8.7273e-03, 9.3910e-01, 5.2170e-02],\n",
       "        [2.2364e-02, 9.4222e-01, 3.5420e-02],\n",
       "        [9.0281e-05, 1.1357e-01, 8.8634e-01],\n",
       "        [9.8669e-01, 1.3306e-02, 8.8764e-09],\n",
       "        [1.2539e-04, 3.3108e-01, 6.6880e-01],\n",
       "        [4.5474e-03, 8.5211e-01, 1.4334e-01],\n",
       "        [8.6302e-03, 9.7365e-01, 1.7720e-02],\n",
       "        [9.7776e-01, 2.2240e-02, 2.4647e-08],\n",
       "        [3.3007e-06, 1.9135e-02, 9.8086e-01],\n",
       "        [7.4464e-05, 1.2274e-01, 8.7718e-01],\n",
       "        [9.8309e-01, 1.6908e-02, 1.9494e-08],\n",
       "        [9.8610e-01, 1.3903e-02, 9.3341e-09],\n",
       "        [1.9269e-02, 9.5758e-01, 2.3154e-02],\n",
       "        [3.6818e-03, 8.6694e-01, 1.2938e-01],\n",
       "        [3.4638e-07, 4.0529e-03, 9.9595e-01],\n",
       "        [9.9887e-01, 1.1330e-03, 5.3234e-12],\n",
       "        [9.8950e-01, 1.0501e-02, 8.5740e-09],\n",
       "        [1.4602e-02, 9.1760e-01, 6.7795e-02],\n",
       "        [9.8467e-01, 1.5335e-02, 1.2347e-08],\n",
       "        [8.8491e-04, 3.8619e-01, 6.1293e-01],\n",
       "        [1.3096e-04, 2.3292e-01, 7.6694e-01],\n",
       "        [9.7792e-01, 2.2082e-02, 3.3456e-08],\n",
       "        [2.9935e-06, 2.4769e-02, 9.7523e-01],\n",
       "        [9.9833e-01, 1.6712e-03, 1.8599e-11],\n",
       "        [9.9423e-01, 5.7673e-03, 3.3201e-10],\n",
       "        [1.2481e-03, 6.7867e-01, 3.2008e-01],\n",
       "        [9.9723e-01, 2.7708e-03, 3.1537e-10],\n",
       "        [9.9558e-01, 4.4232e-03, 2.6127e-10],\n",
       "        [6.1522e-03, 9.4502e-01, 4.8830e-02],\n",
       "        [3.4593e-06, 4.1515e-02, 9.5848e-01],\n",
       "        [1.0938e-02, 9.6423e-01, 2.4834e-02],\n",
       "        [4.8042e-02, 9.3723e-01, 1.4729e-02],\n",
       "        [1.3927e-02, 9.2412e-01, 6.1952e-02],\n",
       "        [9.8062e-01, 1.9376e-02, 3.3710e-08],\n",
       "        [9.9483e-01, 5.1699e-03, 8.1245e-10],\n",
       "        [3.1016e-03, 8.9956e-01, 9.7343e-02],\n",
       "        [1.0924e-05, 4.3231e-02, 9.5676e-01],\n",
       "        [9.8614e-01, 1.3856e-02, 8.2584e-09],\n",
       "        [9.9326e-01, 6.7367e-03, 9.8861e-10],\n",
       "        [5.8345e-03, 7.9748e-01, 1.9669e-01],\n",
       "        [7.7421e-03, 8.7161e-01, 1.2065e-01],\n",
       "        [1.0378e-02, 9.4491e-01, 4.4713e-02],\n",
       "        [9.2133e-06, 5.7502e-02, 9.4249e-01],\n",
       "        [3.4664e-03, 9.1817e-01, 7.8368e-02],\n",
       "        [1.2542e-02, 9.2973e-01, 5.7732e-02],\n",
       "        [1.0534e-02, 8.9692e-01, 9.2551e-02],\n",
       "        [6.0914e-07, 1.5675e-02, 9.8432e-01],\n",
       "        [9.9091e-01, 9.0900e-03, 2.8141e-09],\n",
       "        [9.9529e-01, 4.7139e-03, 3.5664e-10],\n",
       "        [1.0065e-02, 8.9790e-01, 9.2037e-02]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 2, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 2, 0, 0, 1, 0, 0, 2, 0, 2,\n",
       "        0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 1, 1, 2, 0, 0, 1, 1, 0, 2, 2, 2, 2, 2, 1,\n",
       "        0, 0, 2, 0, 0, 1, 1, 1, 1, 2, 1, 2, 0, 2, 1, 0, 0, 2, 1, 2, 2, 0, 1, 1,\n",
       "        2, 0, 2, 1, 1, 0, 2, 2, 0, 0, 1, 1, 2, 0, 0, 1, 0, 1, 2, 0, 2, 0, 0, 1,\n",
       "        0, 0, 1, 2, 1, 1, 1, 0, 0, 1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 0, 0, 1])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n",
      "0.9555555555555556\n",
      "0.9333333333333333\n",
      "0.9393939393939394\n",
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "predict_out = net(torch.tensor(x_test).float())\n",
    "_, y_pred = torch.max(predict_out, 1)\n",
    "\n",
    "print (accuracy_score(y_test, y_pred))\n",
    "\n",
    "print (precision_score(y_test, y_pred, average='macro'))\n",
    "print (precision_score(y_test, y_pred, average='micro'))\n",
    "print (recall_score(y_test, y_pred, average='macro'))\n",
    "print (recall_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5124e-05, 6.2422e-02, 9.3756e-01],\n",
       "        [5.1592e-05, 1.1457e-01, 8.8538e-01],\n",
       "        [3.9500e-05, 1.6542e-01, 8.3454e-01],\n",
       "        [1.3843e-04, 1.9498e-01, 8.0488e-01],\n",
       "        [9.9157e-01, 8.4282e-03, 1.9673e-09],\n",
       "        [1.6890e-03, 4.4774e-01, 5.5057e-01],\n",
       "        [9.9119e-01, 8.8128e-03, 4.2588e-09],\n",
       "        [1.4590e-02, 9.4654e-01, 3.8873e-02],\n",
       "        [2.8205e-03, 6.7344e-01, 3.2374e-01],\n",
       "        [9.9193e-01, 8.0677e-03, 2.1019e-09],\n",
       "        [1.1426e-03, 5.4821e-01, 4.5064e-01],\n",
       "        [7.2897e-06, 3.3557e-02, 9.6644e-01],\n",
       "        [3.4962e-03, 7.6010e-01, 2.3641e-01],\n",
       "        [5.2541e-06, 2.6659e-02, 9.7334e-01],\n",
       "        [3.1956e-05, 8.1120e-02, 9.1885e-01],\n",
       "        [9.9731e-01, 2.6905e-03, 9.5620e-11],\n",
       "        [2.8440e-03, 7.8686e-01, 2.1029e-01],\n",
       "        [9.8745e-01, 1.2552e-02, 1.1724e-08],\n",
       "        [2.2060e-05, 4.8153e-02, 9.5183e-01],\n",
       "        [7.9103e-06, 2.0793e-02, 9.7920e-01],\n",
       "        [9.2416e-03, 8.8334e-01, 1.0742e-01],\n",
       "        [4.3166e-02, 9.2032e-01, 3.6517e-02],\n",
       "        [3.3248e-04, 2.9642e-01, 7.0325e-01],\n",
       "        [4.6221e-05, 4.0420e-02, 9.5953e-01],\n",
       "        [9.5501e-05, 1.3459e-01, 8.6531e-01],\n",
       "        [9.9384e-01, 6.1605e-03, 1.0353e-09],\n",
       "        [1.1519e-01, 8.7788e-01, 6.9281e-03],\n",
       "        [2.4524e-03, 5.7900e-01, 4.1855e-01],\n",
       "        [5.7066e-04, 3.6981e-01, 6.2962e-01],\n",
       "        [5.8483e-06, 4.0454e-02, 9.5954e-01]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 0., 1., 0., 1., 1., 0., 1., 2., 1., 2., 2., 0., 1.,\n",
       "       0., 2., 2., 1., 1., 2., 2., 1., 0., 1., 1., 2., 2.])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.7394770e-04, 4.0819177e-05, 2.2186257e-06, 1.3763828e-02,\n",
       "       9.8616737e-01, 6.2114466e-04, 3.3937348e-03, 9.8896348e-01,\n",
       "       9.7694379e-01, 2.4881689e-03, 3.4207554e-04, 9.9834800e-01,\n",
       "       8.7975385e-03, 5.0197526e-05, 8.1698654e-06, 1.7256619e-06,\n",
       "       9.9654359e-01, 9.9131060e-01, 2.3777692e-03, 9.8334634e-01,\n",
       "       9.9392444e-01, 3.9395283e-04, 9.6809477e-01, 4.6868704e-04,\n",
       "       9.9388039e-01, 9.8817885e-01, 9.9137479e-01, 2.9020704e-05,\n",
       "       4.4036337e-06, 9.9477154e-01, 7.5791954e-06, 5.0870261e-05,\n",
       "       9.7964913e-01, 9.9439114e-01, 1.1319418e-02, 3.9690685e-02,\n",
       "       8.5031854e-05, 9.9303931e-01, 9.9749458e-01, 1.9643961e-02,\n",
       "       1.3970531e-02, 9.9424779e-01, 3.1878965e-06, 8.5061016e-05,\n",
       "       1.6658838e-05, 3.8102742e-06, 5.7934481e-06, 4.9523059e-03,\n",
       "       9.9471259e-01, 9.8502004e-01, 4.2611419e-04, 9.7530431e-01,\n",
       "       9.8616737e-01, 5.1592458e-03, 3.9662262e-03, 7.0456611e-03,\n",
       "       6.1243493e-02, 1.6658838e-05, 1.3780198e-02, 3.7697493e-05,\n",
       "       9.5443636e-01, 2.4206936e-05, 1.2715118e-02, 9.9317944e-01,\n",
       "       9.8609525e-01, 1.0811685e-05, 1.2007697e-02, 3.7512083e-07,\n",
       "       3.4572125e-08, 9.8228925e-01, 8.7100407e-03, 2.2329886e-02,\n",
       "       8.9843772e-05, 9.8671734e-01, 1.2489232e-04, 4.5375144e-03,\n",
       "       8.6130677e-03, 9.7779310e-01, 3.2762043e-06, 7.4095398e-05,\n",
       "       9.8311818e-01, 9.8612100e-01, 1.9236784e-02, 3.6733262e-03,\n",
       "       3.4326447e-07, 9.9886996e-01, 9.8951763e-01, 1.4576928e-02,\n",
       "       9.8469019e-01, 8.8223204e-04, 1.3039065e-04, 9.7795153e-01,\n",
       "       2.9718090e-06, 9.9833292e-01, 9.9424452e-01, 1.2447069e-03,\n",
       "       9.9723560e-01, 9.9558616e-01, 6.1389985e-03, 3.4352786e-06,\n",
       "       1.0916801e-02, 4.7981333e-02, 1.3903405e-02, 9.8065418e-01,\n",
       "       9.9484068e-01, 3.0941230e-03, 1.0855799e-05, 9.8616737e-01,\n",
       "       9.9327660e-01, 5.8227857e-03, 7.7267205e-03, 1.0358189e-02,\n",
       "       9.1548891e-06, 3.4579998e-03, 1.2519664e-02, 1.0514849e-02,\n",
       "       6.0411668e-07, 9.9092698e-01, 9.9529594e-01, 1.0045842e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "#from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "X = Variable(torch.tensor(x_train).float()) \n",
    "result = net(X)\n",
    "pred=result.data[:,0].numpy()\n",
    "pred\n",
    "#print(len(pred),len(y_train))\n",
    "#print (r2_score(pred,y_train))\n",
    "#print (mean_squared_error(pred,y_train))\n",
    "#print (np.mean(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 2., 1., 0., 2., 1., 0., 0., 1., 2., 0., 1., 2., 2., 2., 0.,\n",
       "       0., 1., 0., 0., 2., 0., 2., 0., 0., 0., 2., 2., 0., 2., 2., 0., 0.,\n",
       "       1., 1., 2., 0., 0., 1., 1., 0., 2., 2., 2., 2., 2., 1., 0., 0., 2.,\n",
       "       0., 0., 1., 1., 1., 1., 2., 1., 2., 0., 2., 1., 0., 0., 2., 1., 2.,\n",
       "       2., 0., 1., 1., 2., 0., 2., 1., 1., 0., 2., 2., 0., 0., 1., 1., 2.,\n",
       "       0., 0., 1., 0., 1., 2., 0., 2., 0., 0., 1., 0., 0., 1., 2., 1., 1.,\n",
       "       1., 0., 0., 1., 2., 0., 0., 1., 1., 1., 2., 1., 1., 1., 2., 0., 0.,\n",
       "       1.])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 0, 1, 0, 2, 0, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 0, 1, 2, 0,\n",
       "        0, 2, 1, 2, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1,\n",
       "        1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 0, 2, 1, 0, 2, 1, 0, 0, 0, 1, 0, 1, 2, 0,\n",
       "        0, 2, 0, 2, 1, 0, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 1, 0, 2,\n",
       "        0, 0, 1, 0, 1, 2, 0, 2, 1, 1, 2, 1, 0, 2, 0, 2, 2, 1, 2, 2, 1, 1, 2, 2])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(labels, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7532e-05, 9.9998e-01, 9.9472e-01],\n",
       "        [9.9841e-01, 3.2520e-03, 8.0627e-09],\n",
       "        [9.9872e-01, 2.6295e-03, 6.5938e-09],\n",
       "        [3.0683e-03, 9.9832e-01, 7.6827e-04],\n",
       "        [9.9817e-01, 3.8096e-03, 8.4707e-09],\n",
       "        [5.6918e-05, 9.9995e-01, 9.3180e-01],\n",
       "        [9.9867e-01, 2.7122e-03, 6.9904e-09],\n",
       "        [6.1177e-04, 9.9961e-01, 2.0079e-02],\n",
       "        [9.9835e-01, 3.3730e-03, 8.2017e-09],\n",
       "        [9.9123e-04, 9.9945e-01, 3.1019e-03],\n",
       "        [9.0797e-03, 9.9558e-01, 1.1315e-04],\n",
       "        [4.1772e-03, 9.9784e-01, 4.1455e-04],\n",
       "        [2.2408e-03, 9.9869e-01, 2.0865e-03],\n",
       "        [3.0383e-04, 9.9979e-01, 1.0099e-01],\n",
       "        [2.1925e-05, 9.9998e-01, 9.9087e-01],\n",
       "        [1.1378e-05, 9.9999e-01, 9.9884e-01],\n",
       "        [2.4658e-03, 9.9867e-01, 1.1661e-03],\n",
       "        [9.9862e-01, 2.8383e-03, 7.0753e-09],\n",
       "        [5.8071e-03, 9.9707e-01, 1.8210e-04],\n",
       "        [1.8411e-02, 9.9089e-01, 7.0277e-05],\n",
       "        [9.9581e-01, 8.8533e-03, 1.4452e-08],\n",
       "        [7.1969e-05, 9.9994e-01, 7.3059e-01],\n",
       "        [1.1555e-05, 9.9999e-01, 9.9844e-01],\n",
       "        [9.9816e-01, 3.8226e-03, 8.4895e-09],\n",
       "        [9.9884e-01, 2.3733e-03, 6.2797e-09],\n",
       "        [6.3411e-06, 9.9999e-01, 9.9960e-01],\n",
       "        [6.1805e-03, 9.9691e-01, 1.6756e-04],\n",
       "        [3.4827e-05, 9.9996e-01, 9.7590e-01],\n",
       "        [2.8496e-05, 9.9997e-01, 9.7235e-01],\n",
       "        [9.9847e-01, 3.1592e-03, 7.5884e-09],\n",
       "        [9.9800e-01, 4.1411e-03, 9.1609e-09],\n",
       "        [5.5779e-06, 9.9999e-01, 9.9961e-01],\n",
       "        [9.9881e-01, 2.4329e-03, 6.4025e-09],\n",
       "        [4.9762e-04, 9.9967e-01, 3.7614e-02],\n",
       "        [9.9812e-01, 3.8631e-03, 8.9111e-09],\n",
       "        [9.9862e-01, 2.8458e-03, 7.0485e-09],\n",
       "        [9.9866e-01, 2.7491e-03, 6.9779e-09],\n",
       "        [9.9857e-01, 2.9318e-03, 7.1632e-09],\n",
       "        [2.2631e-04, 9.9982e-01, 3.1646e-01],\n",
       "        [1.0861e-05, 9.9999e-01, 9.9821e-01],\n",
       "        [9.7620e-06, 9.9999e-01, 9.9885e-01],\n",
       "        [1.6489e-05, 9.9998e-01, 9.9552e-01],\n",
       "        [4.8654e-05, 9.9995e-01, 9.4853e-01],\n",
       "        [6.7490e-06, 9.9999e-01, 9.9961e-01],\n",
       "        [1.1869e-05, 9.9999e-01, 9.9865e-01],\n",
       "        [2.5543e-03, 9.9857e-01, 1.3236e-03],\n",
       "        [4.0211e-03, 9.9785e-01, 5.9776e-04],\n",
       "        [3.7119e-04, 9.9975e-01, 6.9938e-02],\n",
       "        [3.8981e-03, 9.9809e-01, 2.5913e-04],\n",
       "        [6.1948e-05, 9.9995e-01, 8.3226e-01],\n",
       "        [9.2052e-03, 9.9546e-01, 8.9897e-05],\n",
       "        [1.6346e-05, 9.9998e-01, 9.9657e-01],\n",
       "        [1.6358e-02, 9.9198e-01, 7.1142e-05],\n",
       "        [9.6509e-06, 9.9999e-01, 9.9897e-01],\n",
       "        [8.5929e-04, 9.9945e-01, 1.2925e-02],\n",
       "        [1.4714e-04, 9.9989e-01, 2.6543e-01],\n",
       "        [8.0714e-06, 9.9999e-01, 9.9920e-01],\n",
       "        [8.2254e-06, 9.9999e-01, 9.9935e-01],\n",
       "        [9.9869e-01, 2.6872e-03, 6.7783e-09],\n",
       "        [3.9233e-05, 9.9996e-01, 9.1441e-01],\n",
       "        [7.6230e-04, 9.9953e-01, 1.2264e-02],\n",
       "        [9.9782e-01, 4.5317e-03, 9.6876e-09],\n",
       "        [1.0040e-05, 9.9999e-01, 9.9823e-01],\n",
       "        [9.9180e-04, 9.9935e-01, 1.7896e-02],\n",
       "        [9.9864e-01, 2.7864e-03, 7.0600e-09],\n",
       "        [9.9777e-01, 4.6049e-03, 1.0100e-08],\n",
       "        [9.9827e-01, 3.5671e-03, 8.4215e-09],\n",
       "        [7.0512e-05, 9.9993e-01, 9.1149e-01],\n",
       "        [9.9762e-01, 4.9403e-03, 1.0547e-08],\n",
       "        [6.3664e-03, 9.9687e-01, 1.7467e-04],\n",
       "        [9.2570e-06, 9.9999e-01, 9.9887e-01],\n",
       "        [9.9819e-01, 3.7267e-03, 8.7480e-09],\n",
       "        [9.9842e-01, 3.2510e-03, 7.7439e-09],\n",
       "        [4.9858e-06, 9.9999e-01, 9.9976e-01],\n",
       "        [9.9773e-01, 4.7031e-03, 1.0131e-08],\n",
       "        [4.4490e-05, 9.9996e-01, 9.5839e-01],\n",
       "        [1.0658e-03, 9.9935e-01, 7.2763e-03],\n",
       "        [9.9844e-01, 3.1994e-03, 7.7610e-09],\n",
       "        [1.7722e-05, 9.9998e-01, 9.9262e-01],\n",
       "        [8.3481e-06, 9.9999e-01, 9.9894e-01],\n",
       "        [4.0982e-03, 9.9788e-01, 3.7540e-04],\n",
       "        [9.0680e-06, 9.9999e-01, 9.9880e-01],\n",
       "        [8.4351e-04, 9.9950e-01, 6.4182e-03],\n",
       "        [9.9827e-01, 3.5465e-03, 8.3869e-09],\n",
       "        [9.9881e-01, 2.4157e-03, 6.3987e-09],\n",
       "        [9.9795e-01, 4.2325e-03, 9.4128e-09],\n",
       "        [9.9858e-01, 2.9111e-03, 7.2583e-09],\n",
       "        [9.9815e-01, 3.7706e-03, 9.0995e-09],\n",
       "        [9.9772e-01, 4.7806e-03, 9.7701e-09],\n",
       "        [7.5849e-06, 9.9999e-01, 9.9925e-01],\n",
       "        [1.3456e-05, 9.9998e-01, 9.9844e-01],\n",
       "        [9.9829e-01, 3.5513e-03, 8.0662e-09],\n",
       "        [1.2222e-03, 9.9931e-01, 2.4785e-03],\n",
       "        [1.8568e-03, 9.9898e-01, 1.3387e-03],\n",
       "        [9.9860e-01, 2.8727e-03, 7.2874e-09],\n",
       "        [1.6330e-05, 9.9998e-01, 9.9600e-01],\n",
       "        [9.9889e-01, 2.2723e-03, 6.1030e-09],\n",
       "        [9.9846e-01, 3.1659e-03, 7.6016e-09],\n",
       "        [9.7867e-03, 9.9514e-01, 1.0186e-04],\n",
       "        [9.9868e-01, 2.7076e-03, 6.8433e-09],\n",
       "        [2.1004e-03, 9.9880e-01, 2.0510e-03],\n",
       "        [7.3134e-06, 9.9999e-01, 9.9950e-01],\n",
       "        [9.9891e-01, 2.2288e-03, 5.8809e-09],\n",
       "        [7.5496e-06, 9.9999e-01, 9.9951e-01],\n",
       "        [9.1626e-04, 9.9943e-01, 1.0312e-02],\n",
       "        [3.6856e-03, 9.9795e-01, 9.0044e-04],\n",
       "        [7.0772e-06, 9.9999e-01, 9.9948e-01],\n",
       "        [3.6827e-04, 9.9975e-01, 5.6246e-02],\n",
       "        [9.9812e-01, 3.8996e-03, 8.6332e-09],\n",
       "        [7.8704e-06, 9.9999e-01, 9.9939e-01],\n",
       "        [9.9796e-01, 4.2159e-03, 9.3907e-09],\n",
       "        [1.0145e-05, 9.9999e-01, 9.9895e-01],\n",
       "        [7.6633e-06, 9.9999e-01, 9.9941e-01],\n",
       "        [5.8769e-04, 9.9964e-01, 1.4009e-02],\n",
       "        [5.8257e-05, 9.9995e-01, 9.0229e-01],\n",
       "        [4.2297e-06, 9.9999e-01, 9.9980e-01],\n",
       "        [4.0576e-03, 9.9790e-01, 4.1391e-04],\n",
       "        [1.9017e-04, 9.9986e-01, 2.0382e-01],\n",
       "        [1.8725e-05, 9.9998e-01, 9.9565e-01],\n",
       "        [2.2968e-05, 9.9998e-01, 9.9108e-01]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
