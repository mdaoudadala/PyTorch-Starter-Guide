{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "def np():\n",
    "\n",
    "    import numpy as np\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    # N is batch size; D_in is input dimension;\n",
    "    # H is hidden dimension; D_out is output dimension.\n",
    "    N, D_in, H, D_out = 64, 10000, 2000, 10\n",
    "\n",
    "    # Create random input and output data\n",
    "    x = np.random.randn(N, D_in)\n",
    "    y = np.random.randn(N, D_out)\n",
    "\n",
    "    # Randomly initialize weights\n",
    "    w1 = np.random.randn(D_in, H)\n",
    "    w2 = np.random.randn(H, D_out)\n",
    "\n",
    "    learning_rate = 1e-6\n",
    "    for t in range(500):\n",
    "        # Forward pass: compute predicted y\n",
    "        h = x.dot(w1)\n",
    "        h_relu = np.maximum(h, 0)\n",
    "        y_pred = h_relu.dot(w2)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = np.square(y_pred - y).sum()\n",
    "        #print(t, loss)\n",
    "\n",
    "        # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "        grad_y_pred = 2.0 * (y_pred - y)\n",
    "        grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "        grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "        grad_h = grad_h_relu.copy()\n",
    "        grad_h[h < 0] = 0\n",
    "        grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "        # Update weights\n",
    "        w1 -= learning_rate * grad_w1\n",
    "        w2 -= learning_rate * grad_w2\n",
    "    print(t, loss)\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "def pt():\n",
    "    import torch\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    dtype = torch.float\n",
    "    device = torch.device(\"cpu\")\n",
    "    # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "    # N is batch size; D_in is input dimension;\n",
    "    # H is hidden dimension; D_out is output dimension.\n",
    "    N, D_in, H, D_out = 64, 10000, 2000, 10\n",
    "\n",
    "    # Create random input and output data\n",
    "    x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "    y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "    # Randomly initialize weights\n",
    "    w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "    w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "    learning_rate = 1e-6\n",
    "    for t in range(500):\n",
    "        # Forward pass: compute predicted y\n",
    "        h = x.mm(w1)\n",
    "        h_relu = h.clamp(min=0)\n",
    "        y_pred = h_relu.mm(w2)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = (y_pred - y).pow(2).sum().item()\n",
    "        #print(t, loss)\n",
    "\n",
    "        # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "        grad_y_pred = 2.0 * (y_pred - y)\n",
    "        grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "        grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "        grad_h = grad_h_relu.clone()\n",
    "        grad_h[h < 0] = 0\n",
    "        grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "        # Update weights using gradient descent\n",
    "        w1 -= learning_rate * grad_w1\n",
    "        w2 -= learning_rate * grad_w2\n",
    "        \n",
    "    end = time.time()\n",
    "    print(t, loss)\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "def pt_f():\n",
    "    import torch\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    dtype = torch.float\n",
    "    device = torch.device(\"cpu\")\n",
    "    # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "    # N is batch size; D_in is input dimension;\n",
    "    # H is hidden dimension; D_out is output dimension.\n",
    "    N, D_in, H, D_out = 64, 10000, 2000, 10\n",
    "\n",
    "    # Create random Tensors to hold input and outputs.\n",
    "    # Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "    # with respect to these Tensors during the backward pass.\n",
    "    x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "    y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "    # Create random Tensors for weights.\n",
    "    # Setting requires_grad=True indicates that we want to compute gradients with\n",
    "    # respect to these Tensors during the backward pass.\n",
    "    w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "    w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "    learning_rate = 1e-6\n",
    "    for t in range(500):\n",
    "        # Forward pass: compute predicted y using operations on Tensors; these\n",
    "        # are exactly the same operations we used to compute the forward pass using\n",
    "        # Tensors, but we do not need to keep references to intermediate values since\n",
    "        # we are not implementing the backward pass by hand.\n",
    "        y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "        # Compute and print loss using operations on Tensors.\n",
    "        # Now loss is a Tensor of shape (1,)\n",
    "        # loss.item() gets the a scalar value held in the loss.\n",
    "        loss = (y_pred - y).pow(2).sum()\n",
    "        #print(t, loss.item())\n",
    "\n",
    "        # Use autograd to compute the backward pass. This call will compute the\n",
    "        # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "        # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "        # of the loss with respect to w1 and w2 respectively.\n",
    "        loss.backward()\n",
    "\n",
    "        # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "        # because weights have requires_grad=True, but we don't need to track this\n",
    "        # in autograd.\n",
    "        # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "        # Recall that tensor.data gives a tensor that shares the storage with\n",
    "        # tensor, but doesn't track history.\n",
    "        # You can also use torch.optim.SGD to achieve this.\n",
    "        with torch.no_grad():\n",
    "            w1 -= learning_rate * w1.grad\n",
    "            w2 -= learning_rate * w2.grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "            w1.grad.zero_()\n",
    "            w2.grad.zero_()\n",
    "    end = time.time()\n",
    "    print(t, loss.item())\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:29: RuntimeWarning: overflow encountered in square\n",
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:37: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 nan\n",
      "163.71753787994385\n",
      "499 nan\n",
      "49.3569073677063\n",
      "499 nan\n",
      "59.15540814399719\n"
     ]
    }
   ],
   "source": [
    "np()\n",
    "pt()\n",
    "pt_f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
