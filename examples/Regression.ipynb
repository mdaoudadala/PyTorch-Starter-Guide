{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://msdn.microsoft.com/en-us/magazine/mt833293.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://medium.com/coinmonks/create-a-neural-network-in-pytorch-and-make-your-life-simpler-ec5367895199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set numpy seed for reproducibility\n",
    "np.random.seed(seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "    black  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../data/boston.csv\", sep=',')[['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'black', 'lstat', 'medv']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crim       float64\n",
       "zn         float64\n",
       "indus      float64\n",
       "chas         int64\n",
       "nox        float64\n",
       "rm         float64\n",
       "age        float64\n",
       "dis        float64\n",
       "rad          int64\n",
       "tax          int64\n",
       "ptratio    float64\n",
       "black      float64\n",
       "lstat      float64\n",
       "medv       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n",
       "        6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02,\n",
       "        1.5300e+01, 3.9690e+02, 4.9800e+00, 2.4000e+01],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "        6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "        1.7800e+01, 3.9690e+02, 9.1400e+00, 2.1600e+01]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.values\n",
    "dataset[:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 506. Train size: 253 - Test size: 253\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.5\n",
    "train_size = int(len(dataset) * train_size)\n",
    "np.random.shuffle(dataset)\n",
    "train, test = dataset[:train_size,:], dataset[train_size:,:]\n",
    "print ('Dataset size: {0}. Train size: {1} - Test size: {2}'.format(len(dataset), len(train), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and set data into numpy nd.array\n",
    "x_train = train[:,:-1]\n",
    "x_test = test[:,:-1]\n",
    "y_train = train[:,-1]\n",
    "y_test = test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(x_train) # 50\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "size_hidden = 100\n",
    "n_feature = 13\n",
    "n_output = 1\n",
    "batch_no = int(len(x_train) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the model on : cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnet = torch.nn.Sequential(\\n    torch.nn.Linear(n_feature, size_hidden),\\n    torch.nn.Sigmoid(),\\n    torch.nn.Linear(size_hidden, n_output),\\n)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "print(\"Executing the model on :\",device)\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, size_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_1 = torch.nn.Linear(n_feature, size_hidden)   # hidden layer\n",
    "        self.hidden_2 = torch.nn.Linear(size_hidden, size_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(size_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.hidden_1(x))      # activation function for hidden layer\n",
    "        x = F.sigmoid(self.hidden_2(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "net = Net(n_feature, size_hidden, n_output)\n",
    "\n",
    "\"\"\"\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_feature, size_hidden),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(size_hidden, n_output),\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam is a specific flavor of gradient decent which is typically better\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "#criterion = torch.nn.MSELoss(size_average=False)  # this is for regression mean squared loss\n",
    "criterion = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss:  150410.609375\n",
      "Epoch 2 loss:  137939.109375\n",
      "Epoch 3 loss:  126483.4140625\n",
      "Epoch 4 loss:  116403.6328125\n",
      "Epoch 5 loss:  107722.9921875\n",
      "Epoch 6 loss:  100246.984375\n",
      "Epoch 7 loss:  93654.171875\n",
      "Epoch 8 loss:  87638.28125\n",
      "Epoch 9 loss:  81977.375\n",
      "Epoch 10 loss:  76602.46875\n",
      "Epoch 11 loss:  71501.5\n",
      "Epoch 12 loss:  66686.078125\n",
      "Epoch 13 loss:  62169.359375\n",
      "Epoch 14 loss:  57964.015625\n",
      "Epoch 15 loss:  54042.890625\n",
      "Epoch 16 loss:  50370.68359375\n",
      "Epoch 17 loss:  46931.671875\n",
      "Epoch 18 loss:  43731.2890625\n",
      "Epoch 19 loss:  40783.27734375\n",
      "Epoch 20 loss:  38086.359375\n",
      "Epoch 21 loss:  35631.32421875\n",
      "Epoch 22 loss:  33409.42578125\n",
      "Epoch 23 loss:  31418.810546875\n",
      "Epoch 24 loss:  29650.369140625\n",
      "Epoch 25 loss:  28087.447265625\n",
      "Epoch 26 loss:  26715.1484375\n",
      "Epoch 27 loss:  25525.00390625\n",
      "Epoch 28 loss:  24511.166015625\n",
      "Epoch 29 loss:  23663.97265625\n",
      "Epoch 30 loss:  22968.80078125\n",
      "Epoch 31 loss:  22412.486328125\n",
      "Epoch 32 loss:  21982.546875\n",
      "Epoch 33 loss:  21664.6015625\n",
      "Epoch 34 loss:  21441.412109375\n",
      "Epoch 35 loss:  21297.02734375\n",
      "Epoch 36 loss:  21217.869140625\n",
      "Epoch 37 loss:  21192.05859375\n",
      "Epoch 38 loss:  21205.828125\n",
      "Epoch 39 loss:  21249.25\n",
      "Epoch 40 loss:  21314.80078125\n",
      "Epoch 41 loss:  21392.845703125\n",
      "Epoch 42 loss:  21475.84765625\n",
      "Epoch 43 loss:  21557.087890625\n",
      "Epoch 44 loss:  21632.314453125\n",
      "Epoch 45 loss:  21695.669921875\n",
      "Epoch 46 loss:  21745.01953125\n",
      "Epoch 47 loss:  21779.060546875\n",
      "Epoch 48 loss:  21797.5234375\n",
      "Epoch 49 loss:  21801.00390625\n",
      "Epoch 50 loss:  21791.044921875\n",
      "Epoch 51 loss:  21769.833984375\n",
      "Epoch 52 loss:  21739.353515625\n",
      "Epoch 53 loss:  21700.849609375\n",
      "Epoch 54 loss:  21657.166015625\n",
      "Epoch 55 loss:  21605.064453125\n",
      "Epoch 56 loss:  21553.23046875\n",
      "Epoch 57 loss:  21500.755859375\n",
      "Epoch 58 loss:  21446.091796875\n",
      "Epoch 59 loss:  21392.419921875\n",
      "Epoch 60 loss:  21338.828125\n",
      "Epoch 61 loss:  21285.396484375\n",
      "Epoch 62 loss:  21233.169921875\n",
      "Epoch 63 loss:  21184.38671875\n",
      "Epoch 64 loss:  21135.5625\n",
      "Epoch 65 loss:  21087.84375\n",
      "Epoch 66 loss:  21035.298828125\n",
      "Epoch 67 loss:  20984.037109375\n",
      "Epoch 68 loss:  20943.287109375\n",
      "Epoch 69 loss:  20901.744140625\n",
      "Epoch 70 loss:  20862.912109375\n",
      "Epoch 71 loss:  20825.1796875\n",
      "Epoch 72 loss:  20788.28515625\n",
      "Epoch 73 loss:  20751.216796875\n",
      "Epoch 74 loss:  20704.978515625\n",
      "Epoch 75 loss:  20655.994140625\n",
      "Epoch 76 loss:  20597.865234375\n",
      "Epoch 77 loss:  20534.576171875\n",
      "Epoch 78 loss:  20475.580078125\n",
      "Epoch 79 loss:  20417.150390625\n",
      "Epoch 80 loss:  20349.685546875\n",
      "Epoch 81 loss:  20265.86328125\n",
      "Epoch 82 loss:  20202.8671875\n",
      "Epoch 83 loss:  20137.19921875\n",
      "Epoch 84 loss:  20074.517578125\n",
      "Epoch 85 loss:  20016.12890625\n",
      "Epoch 86 loss:  19949.01953125\n",
      "Epoch 87 loss:  19872.37109375\n",
      "Epoch 88 loss:  19791.69140625\n",
      "Epoch 89 loss:  19699.5078125\n",
      "Epoch 90 loss:  19622.861328125\n",
      "Epoch 91 loss:  19528.96484375\n",
      "Epoch 92 loss:  19448.873046875\n",
      "Epoch 93 loss:  19361.416015625\n",
      "Epoch 94 loss:  19287.154296875\n",
      "Epoch 95 loss:  19204.63671875\n",
      "Epoch 96 loss:  19103.646484375\n",
      "Epoch 97 loss:  19012.27734375\n",
      "Epoch 98 loss:  18945.994140625\n",
      "Epoch 99 loss:  18859.345703125\n",
      "Epoch 100 loss:  18753.892578125\n",
      "Epoch 101 loss:  18649.607421875\n",
      "Epoch 102 loss:  18559.427734375\n",
      "Epoch 103 loss:  18490.34765625\n",
      "Epoch 104 loss:  18372.443359375\n",
      "Epoch 105 loss:  18301.595703125\n",
      "Epoch 106 loss:  18202.193359375\n",
      "Epoch 107 loss:  18099.103515625\n",
      "Epoch 108 loss:  17972.548828125\n",
      "Epoch 109 loss:  17897.0703125\n",
      "Epoch 110 loss:  17778.17578125\n",
      "Epoch 111 loss:  17637.33984375\n",
      "Epoch 112 loss:  17545.755859375\n",
      "Epoch 113 loss:  17393.453125\n",
      "Epoch 114 loss:  17298.31640625\n",
      "Epoch 115 loss:  17147.30078125\n",
      "Epoch 116 loss:  16991.9140625\n",
      "Epoch 117 loss:  16867.2578125\n",
      "Epoch 118 loss:  16717.220703125\n",
      "Epoch 119 loss:  16617.95703125\n",
      "Epoch 120 loss:  16441.44921875\n",
      "Epoch 121 loss:  16337.80859375\n",
      "Epoch 122 loss:  16191.7265625\n",
      "Epoch 123 loss:  16031.2275390625\n",
      "Epoch 124 loss:  15961.7705078125\n",
      "Epoch 125 loss:  15790.701171875\n",
      "Epoch 126 loss:  15656.01171875\n",
      "Epoch 127 loss:  15525.9453125\n",
      "Epoch 128 loss:  15405.9794921875\n",
      "Epoch 129 loss:  15262.6708984375\n",
      "Epoch 130 loss:  15114.93359375\n",
      "Epoch 131 loss:  15017.6845703125\n",
      "Epoch 132 loss:  14883.8935546875\n",
      "Epoch 133 loss:  14680.951171875\n",
      "Epoch 134 loss:  14632.1689453125\n",
      "Epoch 135 loss:  14444.4208984375\n",
      "Epoch 136 loss:  14232.3916015625\n",
      "Epoch 137 loss:  14036.3408203125\n",
      "Epoch 138 loss:  13884.5771484375\n",
      "Epoch 139 loss:  13641.5751953125\n",
      "Epoch 140 loss:  13549.7177734375\n",
      "Epoch 141 loss:  13667.7373046875\n",
      "Epoch 142 loss:  13283.33203125\n",
      "Epoch 143 loss:  13221.6787109375\n",
      "Epoch 144 loss:  13003.755859375\n",
      "Epoch 145 loss:  12862.140625\n",
      "Epoch 146 loss:  12649.1962890625\n",
      "Epoch 147 loss:  12493.369140625\n",
      "Epoch 148 loss:  12340.3544921875\n",
      "Epoch 149 loss:  12101.2001953125\n",
      "Epoch 150 loss:  11996.947265625\n",
      "Epoch 151 loss:  11747.5068359375\n",
      "Epoch 152 loss:  11469.92578125\n",
      "Epoch 153 loss:  11297.2939453125\n",
      "Epoch 154 loss:  11251.248046875\n",
      "Epoch 155 loss:  10862.650390625\n",
      "Epoch 156 loss:  10580.9345703125\n",
      "Epoch 157 loss:  10545.47265625\n",
      "Epoch 158 loss:  10270.4765625\n",
      "Epoch 159 loss:  10075.3310546875\n",
      "Epoch 160 loss:  10191.2626953125\n",
      "Epoch 161 loss:  9991.5205078125\n",
      "Epoch 162 loss:  9625.501953125\n",
      "Epoch 163 loss:  9691.5791015625\n",
      "Epoch 164 loss:  9568.138671875\n",
      "Epoch 165 loss:  9417.6318359375\n",
      "Epoch 166 loss:  9060.2998046875\n",
      "Epoch 167 loss:  8912.4345703125\n",
      "Epoch 168 loss:  8923.578125\n",
      "Epoch 169 loss:  8674.4775390625\n",
      "Epoch 170 loss:  8680.8212890625\n",
      "Epoch 171 loss:  8418.58984375\n",
      "Epoch 172 loss:  8195.4150390625\n",
      "Epoch 173 loss:  8150.38525390625\n",
      "Epoch 174 loss:  8421.35546875\n",
      "Epoch 175 loss:  7802.29345703125\n",
      "Epoch 176 loss:  8597.6865234375\n",
      "Epoch 177 loss:  8428.3115234375\n",
      "Epoch 178 loss:  8188.65234375\n",
      "Epoch 179 loss:  8138.6513671875\n",
      "Epoch 180 loss:  7953.55712890625\n",
      "Epoch 181 loss:  8261.0322265625\n",
      "Epoch 182 loss:  8362.7626953125\n",
      "Epoch 183 loss:  7679.7333984375\n",
      "Epoch 184 loss:  8124.43359375\n",
      "Epoch 185 loss:  7769.42822265625\n",
      "Epoch 186 loss:  7503.33349609375\n",
      "Epoch 187 loss:  7453.36376953125\n",
      "Epoch 188 loss:  7186.234375\n",
      "Epoch 189 loss:  7357.291015625\n",
      "Epoch 190 loss:  7020.95458984375\n",
      "Epoch 191 loss:  6975.43896484375\n",
      "Epoch 192 loss:  6831.0478515625\n",
      "Epoch 193 loss:  6872.666015625\n",
      "Epoch 194 loss:  6891.345703125\n",
      "Epoch 195 loss:  6636.54931640625\n",
      "Epoch 196 loss:  6708.70361328125\n",
      "Epoch 197 loss:  6492.298828125\n",
      "Epoch 198 loss:  6655.40185546875\n",
      "Epoch 199 loss:  6381.90185546875\n",
      "Epoch 200 loss:  6435.6298828125\n",
      "Epoch 201 loss:  6279.1142578125\n",
      "Epoch 202 loss:  6309.6240234375\n",
      "Epoch 203 loss:  6223.68994140625\n",
      "Epoch 204 loss:  6151.1025390625\n",
      "Epoch 205 loss:  6111.0986328125\n",
      "Epoch 206 loss:  6016.95849609375\n",
      "Epoch 207 loss:  6031.30322265625\n",
      "Epoch 208 loss:  5893.0595703125\n",
      "Epoch 209 loss:  5874.54541015625\n",
      "Epoch 210 loss:  5818.0537109375\n",
      "Epoch 211 loss:  5763.97998046875\n",
      "Epoch 212 loss:  5707.869140625\n",
      "Epoch 213 loss:  5616.00830078125\n",
      "Epoch 214 loss:  5600.50146484375\n",
      "Epoch 215 loss:  5552.82373046875\n",
      "Epoch 216 loss:  5466.09814453125\n",
      "Epoch 217 loss:  5461.57470703125\n",
      "Epoch 218 loss:  5429.32666015625\n",
      "Epoch 219 loss:  5353.3525390625\n",
      "Epoch 220 loss:  5326.78125\n",
      "Epoch 221 loss:  5301.49365234375\n",
      "Epoch 222 loss:  5233.220703125\n",
      "Epoch 223 loss:  5189.34423828125\n",
      "Epoch 224 loss:  5163.73974609375\n",
      "Epoch 225 loss:  5109.986328125\n",
      "Epoch 226 loss:  5060.5078125\n",
      "Epoch 227 loss:  5031.2412109375\n",
      "Epoch 228 loss:  4988.07470703125\n",
      "Epoch 229 loss:  4962.58642578125\n",
      "Epoch 230 loss:  4918.3642578125\n",
      "Epoch 231 loss:  4881.3115234375\n",
      "Epoch 232 loss:  4854.78857421875\n",
      "Epoch 233 loss:  4821.77685546875\n",
      "Epoch 234 loss:  4784.2548828125\n",
      "Epoch 235 loss:  4755.75244140625\n",
      "Epoch 236 loss:  4723.07568359375\n",
      "Epoch 237 loss:  4690.435546875\n",
      "Epoch 238 loss:  4660.6455078125\n",
      "Epoch 239 loss:  4625.59619140625\n",
      "Epoch 240 loss:  4596.13720703125\n",
      "Epoch 241 loss:  4567.88818359375\n",
      "Epoch 242 loss:  4536.455078125\n",
      "Epoch 243 loss:  4507.388671875\n",
      "Epoch 244 loss:  4479.0224609375\n",
      "Epoch 245 loss:  4450.20166015625\n",
      "Epoch 246 loss:  4423.96142578125\n",
      "Epoch 247 loss:  4397.33984375\n",
      "Epoch 248 loss:  4369.4755859375\n",
      "Epoch 249 loss:  4343.64501953125\n",
      "Epoch 250 loss:  4316.1669921875\n",
      "Epoch 251 loss:  4290.39794921875\n",
      "Epoch 252 loss:  4265.2763671875\n",
      "Epoch 253 loss:  4239.23828125\n",
      "Epoch 254 loss:  4211.20654296875\n",
      "Epoch 255 loss:  4191.9560546875\n",
      "Epoch 256 loss:  4167.80712890625\n",
      "Epoch 257 loss:  4144.6748046875\n",
      "Epoch 258 loss:  4122.462890625\n",
      "Epoch 259 loss:  4102.2587890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 260 loss:  4087.367431640625\n",
      "Epoch 261 loss:  4077.43408203125\n",
      "Epoch 262 loss:  4086.719482421875\n",
      "Epoch 263 loss:  4081.40625\n",
      "Epoch 264 loss:  4095.349365234375\n",
      "Epoch 265 loss:  4051.682861328125\n",
      "Epoch 266 loss:  4021.933837890625\n",
      "Epoch 267 loss:  3952.135498046875\n",
      "Epoch 268 loss:  3913.568603515625\n",
      "Epoch 269 loss:  3898.241455078125\n",
      "Epoch 270 loss:  3901.692138671875\n",
      "Epoch 271 loss:  3923.19873046875\n",
      "Epoch 272 loss:  3900.663818359375\n",
      "Epoch 273 loss:  3913.958984375\n",
      "Epoch 274 loss:  3876.8115234375\n",
      "Epoch 275 loss:  3845.514892578125\n",
      "Epoch 276 loss:  3795.79931640625\n",
      "Epoch 277 loss:  3794.5966796875\n",
      "Epoch 278 loss:  3891.339111328125\n",
      "Epoch 279 loss:  4252.53466796875\n",
      "Epoch 280 loss:  4268.76318359375\n",
      "Epoch 281 loss:  3791.835693359375\n",
      "Epoch 282 loss:  4339.84912109375\n",
      "Epoch 283 loss:  4407.80712890625\n",
      "Epoch 284 loss:  4120.7314453125\n",
      "Epoch 285 loss:  4420.09521484375\n",
      "Epoch 286 loss:  4256.6845703125\n",
      "Epoch 287 loss:  4413.9033203125\n",
      "Epoch 288 loss:  4115.5224609375\n",
      "Epoch 289 loss:  4274.5556640625\n",
      "Epoch 290 loss:  4220.7412109375\n",
      "Epoch 291 loss:  3949.761962890625\n",
      "Epoch 292 loss:  4163.1640625\n",
      "Epoch 293 loss:  4015.173095703125\n",
      "Epoch 294 loss:  3945.54248046875\n",
      "Epoch 295 loss:  4114.0654296875\n",
      "Epoch 296 loss:  3954.04150390625\n",
      "Epoch 297 loss:  3900.455810546875\n",
      "Epoch 298 loss:  3966.96533203125\n",
      "Epoch 299 loss:  3830.721923828125\n",
      "Epoch 300 loss:  3874.073974609375\n",
      "Epoch 301 loss:  3848.54052734375\n",
      "Epoch 302 loss:  3755.7724609375\n",
      "Epoch 303 loss:  3823.404052734375\n",
      "Epoch 304 loss:  3765.856201171875\n",
      "Epoch 305 loss:  3701.669921875\n",
      "Epoch 306 loss:  3732.155029296875\n",
      "Epoch 307 loss:  3699.505126953125\n",
      "Epoch 308 loss:  3643.2666015625\n",
      "Epoch 309 loss:  3655.794189453125\n",
      "Epoch 310 loss:  3625.691650390625\n",
      "Epoch 311 loss:  3596.720703125\n",
      "Epoch 312 loss:  3585.619140625\n",
      "Epoch 313 loss:  3577.3017578125\n",
      "Epoch 314 loss:  3559.35546875\n",
      "Epoch 315 loss:  3546.71337890625\n",
      "Epoch 316 loss:  3543.248779296875\n",
      "Epoch 317 loss:  3520.86083984375\n",
      "Epoch 318 loss:  3503.21533203125\n",
      "Epoch 319 loss:  3500.486083984375\n",
      "Epoch 320 loss:  3488.860107421875\n",
      "Epoch 321 loss:  3472.21337890625\n",
      "Epoch 322 loss:  3446.998779296875\n",
      "Epoch 323 loss:  3442.756591796875\n",
      "Epoch 324 loss:  3445.21044921875\n",
      "Epoch 325 loss:  3434.2587890625\n",
      "Epoch 326 loss:  3417.6591796875\n",
      "Epoch 327 loss:  3396.163330078125\n",
      "Epoch 328 loss:  3381.754638671875\n",
      "Epoch 329 loss:  3376.057861328125\n",
      "Epoch 330 loss:  3375.946044921875\n",
      "Epoch 331 loss:  3361.6533203125\n",
      "Epoch 332 loss:  3347.278076171875\n",
      "Epoch 333 loss:  3332.082763671875\n",
      "Epoch 334 loss:  3321.703125\n",
      "Epoch 335 loss:  3314.122802734375\n",
      "Epoch 336 loss:  3310.850341796875\n",
      "Epoch 337 loss:  3301.74462890625\n",
      "Epoch 338 loss:  3291.2275390625\n",
      "Epoch 339 loss:  3276.075927734375\n",
      "Epoch 340 loss:  3263.619873046875\n",
      "Epoch 341 loss:  3252.545166015625\n",
      "Epoch 342 loss:  3245.25\n",
      "Epoch 343 loss:  3237.140625\n",
      "Epoch 344 loss:  3232.18701171875\n",
      "Epoch 345 loss:  3227.559814453125\n",
      "Epoch 346 loss:  3226.79638671875\n",
      "Epoch 347 loss:  3227.67529296875\n",
      "Epoch 348 loss:  3236.861083984375\n",
      "Epoch 349 loss:  3247.34521484375\n",
      "Epoch 350 loss:  3261.819580078125\n",
      "Epoch 351 loss:  3242.112060546875\n",
      "Epoch 352 loss:  3202.619140625\n",
      "Epoch 353 loss:  3166.062744140625\n",
      "Epoch 354 loss:  3143.4443359375\n",
      "Epoch 355 loss:  3132.696044921875\n",
      "Epoch 356 loss:  3129.06298828125\n",
      "Epoch 357 loss:  3132.884033203125\n",
      "Epoch 358 loss:  3145.2373046875\n",
      "Epoch 359 loss:  3169.87109375\n",
      "Epoch 360 loss:  3188.434326171875\n",
      "Epoch 361 loss:  3179.629638671875\n",
      "Epoch 362 loss:  3145.449951171875\n",
      "Epoch 363 loss:  3107.1435546875\n",
      "Epoch 364 loss:  3074.468505859375\n",
      "Epoch 365 loss:  3057.857177734375\n",
      "Epoch 366 loss:  3055.084228515625\n",
      "Epoch 367 loss:  3063.86328125\n",
      "Epoch 368 loss:  3085.470703125\n",
      "Epoch 369 loss:  3115.499267578125\n",
      "Epoch 370 loss:  3138.77783203125\n",
      "Epoch 371 loss:  3145.397705078125\n",
      "Epoch 372 loss:  3100.175537109375\n",
      "Epoch 373 loss:  3041.682373046875\n",
      "Epoch 374 loss:  3001.531982421875\n",
      "Epoch 375 loss:  2985.21826171875\n",
      "Epoch 376 loss:  2987.369140625\n",
      "Epoch 377 loss:  3002.5048828125\n",
      "Epoch 378 loss:  3022.03759765625\n",
      "Epoch 379 loss:  3033.7978515625\n",
      "Epoch 380 loss:  3038.45849609375\n",
      "Epoch 381 loss:  3010.474609375\n",
      "Epoch 382 loss:  2976.222412109375\n",
      "Epoch 383 loss:  2941.106201171875\n",
      "Epoch 384 loss:  2916.048828125\n",
      "Epoch 385 loss:  2898.192138671875\n",
      "Epoch 386 loss:  2885.123779296875\n",
      "Epoch 387 loss:  2875.38623046875\n",
      "Epoch 388 loss:  2868.482666015625\n",
      "Epoch 389 loss:  2866.3349609375\n",
      "Epoch 390 loss:  2872.52001953125\n",
      "Epoch 391 loss:  2899.513916015625\n",
      "Epoch 392 loss:  2938.621337890625\n",
      "Epoch 393 loss:  3009.67529296875\n",
      "Epoch 394 loss:  2966.894287109375\n",
      "Epoch 395 loss:  2903.010986328125\n",
      "Epoch 396 loss:  2824.072021484375\n",
      "Epoch 397 loss:  2786.19677734375\n",
      "Epoch 398 loss:  2781.851318359375\n",
      "Epoch 399 loss:  2804.140625\n",
      "Epoch 400 loss:  2850.065185546875\n",
      "Epoch 401 loss:  2869.2783203125\n",
      "Epoch 402 loss:  2894.1064453125\n",
      "Epoch 403 loss:  2819.63232421875\n",
      "Epoch 404 loss:  2767.68115234375\n",
      "Epoch 405 loss:  2734.11572265625\n",
      "Epoch 406 loss:  2722.504638671875\n",
      "Epoch 407 loss:  2727.3095703125\n",
      "Epoch 408 loss:  2742.3349609375\n",
      "Epoch 409 loss:  2768.10595703125\n",
      "Epoch 410 loss:  2778.93408203125\n",
      "Epoch 411 loss:  2796.50048828125\n",
      "Epoch 412 loss:  2763.397216796875\n",
      "Epoch 413 loss:  2739.812255859375\n",
      "Epoch 414 loss:  2704.300048828125\n",
      "Epoch 415 loss:  2683.33447265625\n",
      "Epoch 416 loss:  2668.427001953125\n",
      "Epoch 417 loss:  2658.67236328125\n",
      "Epoch 418 loss:  2652.20654296875\n",
      "Epoch 419 loss:  2647.1962890625\n",
      "Epoch 420 loss:  2644.662109375\n",
      "Epoch 421 loss:  2644.340576171875\n",
      "Epoch 422 loss:  2651.5654296875\n",
      "Epoch 423 loss:  2665.622802734375\n",
      "Epoch 424 loss:  2706.6201171875\n",
      "Epoch 425 loss:  2732.236083984375\n",
      "Epoch 426 loss:  2784.0048828125\n",
      "Epoch 427 loss:  2715.724609375\n",
      "Epoch 428 loss:  2665.825927734375\n",
      "Epoch 429 loss:  2611.3076171875\n",
      "Epoch 430 loss:  2588.80029296875\n",
      "Epoch 431 loss:  2591.37060546875\n",
      "Epoch 432 loss:  2610.968017578125\n",
      "Epoch 433 loss:  2650.126953125\n",
      "Epoch 434 loss:  2653.009521484375\n",
      "Epoch 435 loss:  2668.755126953125\n",
      "Epoch 436 loss:  2615.94091796875\n",
      "Epoch 437 loss:  2583.995849609375\n",
      "Epoch 438 loss:  2555.881103515625\n",
      "Epoch 439 loss:  2542.7392578125\n",
      "Epoch 440 loss:  2538.85400390625\n",
      "Epoch 441 loss:  2542.177001953125\n",
      "Epoch 442 loss:  2553.252197265625\n",
      "Epoch 443 loss:  2564.368896484375\n",
      "Epoch 444 loss:  2589.7314453125\n",
      "Epoch 445 loss:  2588.15380859375\n",
      "Epoch 446 loss:  2605.62939453125\n",
      "Epoch 447 loss:  2568.79345703125\n",
      "Epoch 448 loss:  2548.178466796875\n",
      "Epoch 449 loss:  2516.333251953125\n",
      "Epoch 450 loss:  2496.818603515625\n",
      "Epoch 451 loss:  2485.251220703125\n",
      "Epoch 452 loss:  2481.130615234375\n",
      "Epoch 453 loss:  2482.9814453125\n",
      "Epoch 454 loss:  2489.160400390625\n",
      "Epoch 455 loss:  2505.32470703125\n",
      "Epoch 456 loss:  2516.65625\n",
      "Epoch 457 loss:  2549.988037109375\n",
      "Epoch 458 loss:  2538.308837890625\n",
      "Epoch 459 loss:  2543.599365234375\n",
      "Epoch 460 loss:  2496.515380859375\n",
      "Epoch 461 loss:  2464.0478515625\n",
      "Epoch 462 loss:  2439.2998046875\n",
      "Epoch 463 loss:  2431.016357421875\n",
      "Epoch 464 loss:  2435.7666015625\n",
      "Epoch 465 loss:  2446.809326171875\n",
      "Epoch 466 loss:  2465.67626953125\n",
      "Epoch 467 loss:  2464.147705078125\n",
      "Epoch 468 loss:  2471.618408203125\n",
      "Epoch 469 loss:  2446.90087890625\n",
      "Epoch 470 loss:  2431.024169921875\n",
      "Epoch 471 loss:  2409.27099609375\n",
      "Epoch 472 loss:  2395.19140625\n",
      "Epoch 473 loss:  2386.772705078125\n",
      "Epoch 474 loss:  2383.6376953125\n",
      "Epoch 475 loss:  2384.793212890625\n",
      "Epoch 476 loss:  2388.68896484375\n",
      "Epoch 477 loss:  2399.0556640625\n",
      "Epoch 478 loss:  2406.44970703125\n",
      "Epoch 479 loss:  2428.139404296875\n",
      "Epoch 480 loss:  2422.651611328125\n",
      "Epoch 481 loss:  2430.23095703125\n",
      "Epoch 482 loss:  2399.337890625\n",
      "Epoch 483 loss:  2377.79150390625\n",
      "Epoch 484 loss:  2353.806396484375\n",
      "Epoch 485 loss:  2340.089111328125\n",
      "Epoch 486 loss:  2334.968994140625\n",
      "Epoch 487 loss:  2336.790771484375\n",
      "Epoch 488 loss:  2344.88134765625\n",
      "Epoch 489 loss:  2354.13720703125\n",
      "Epoch 490 loss:  2376.040771484375\n",
      "Epoch 491 loss:  2379.036376953125\n",
      "Epoch 492 loss:  2399.640869140625\n",
      "Epoch 493 loss:  2371.999267578125\n",
      "Epoch 494 loss:  2345.945556640625\n",
      "Epoch 495 loss:  2353.95849609375\n",
      "Epoch 496 loss:  2318.03369140625\n",
      "Epoch 497 loss:  2318.893798828125\n",
      "Epoch 498 loss:  2297.8447265625\n",
      "Epoch 499 loss:  2283.156982421875\n",
      "Epoch 500 loss:  2271.143798828125\n",
      "Epoch 501 loss:  2260.987548828125\n",
      "Epoch 502 loss:  2254.213134765625\n",
      "Epoch 503 loss:  2247.6298828125\n",
      "Epoch 504 loss:  2241.798583984375\n",
      "Epoch 505 loss:  2242.9208984375\n",
      "Epoch 506 loss:  2241.83740234375\n",
      "Epoch 507 loss:  2249.7880859375\n",
      "Epoch 508 loss:  2251.469482421875\n",
      "Epoch 509 loss:  2264.83203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 510 loss:  2268.181396484375\n",
      "Epoch 511 loss:  2293.474853515625\n",
      "Epoch 512 loss:  2257.363037109375\n",
      "Epoch 513 loss:  2237.282958984375\n",
      "Epoch 514 loss:  2201.90185546875\n",
      "Epoch 515 loss:  2184.5849609375\n",
      "Epoch 516 loss:  2174.304931640625\n",
      "Epoch 517 loss:  2168.22802734375\n",
      "Epoch 518 loss:  2173.481689453125\n",
      "Epoch 519 loss:  2174.379638671875\n",
      "Epoch 520 loss:  2193.025390625\n",
      "Epoch 521 loss:  2195.571044921875\n",
      "Epoch 522 loss:  2239.461181640625\n",
      "Epoch 523 loss:  2252.49853515625\n",
      "Epoch 524 loss:  2372.90869140625\n",
      "Epoch 525 loss:  2307.03857421875\n",
      "Epoch 526 loss:  2187.495361328125\n",
      "Epoch 527 loss:  2164.221435546875\n",
      "Epoch 528 loss:  2131.458251953125\n",
      "Epoch 529 loss:  2158.494873046875\n",
      "Epoch 530 loss:  2136.200927734375\n",
      "Epoch 531 loss:  2125.039306640625\n",
      "Epoch 532 loss:  2112.86328125\n",
      "Epoch 533 loss:  2173.14306640625\n",
      "Epoch 534 loss:  2220.162109375\n",
      "Epoch 535 loss:  2162.458251953125\n",
      "Epoch 536 loss:  2634.70849609375\n",
      "Epoch 537 loss:  2807.440673828125\n",
      "Epoch 538 loss:  2898.534423828125\n",
      "Epoch 539 loss:  2455.8720703125\n",
      "Epoch 540 loss:  2372.6142578125\n",
      "Epoch 541 loss:  3018.78564453125\n",
      "Epoch 542 loss:  3273.63427734375\n",
      "Epoch 543 loss:  3159.126953125\n",
      "Epoch 544 loss:  2498.5888671875\n",
      "Epoch 545 loss:  2862.95068359375\n",
      "Epoch 546 loss:  2941.44482421875\n",
      "Epoch 547 loss:  2514.1650390625\n",
      "Epoch 548 loss:  2804.84033203125\n",
      "Epoch 549 loss:  2822.540771484375\n",
      "Epoch 550 loss:  2779.044921875\n",
      "Epoch 551 loss:  3001.529052734375\n",
      "Epoch 552 loss:  3231.244140625\n",
      "Epoch 553 loss:  2559.414794921875\n",
      "Epoch 554 loss:  2885.471923828125\n",
      "Epoch 555 loss:  3205.480712890625\n",
      "Epoch 556 loss:  2650.345947265625\n",
      "Epoch 557 loss:  2989.951904296875\n",
      "Epoch 558 loss:  2888.112548828125\n",
      "Epoch 559 loss:  2708.19384765625\n",
      "Epoch 560 loss:  2685.464599609375\n",
      "Epoch 561 loss:  2797.93115234375\n",
      "Epoch 562 loss:  2629.432861328125\n",
      "Epoch 563 loss:  2749.654296875\n",
      "Epoch 564 loss:  2689.57568359375\n",
      "Epoch 565 loss:  2789.9716796875\n",
      "Epoch 566 loss:  2670.711669921875\n",
      "Epoch 567 loss:  2411.886474609375\n",
      "Epoch 568 loss:  2162.968017578125\n",
      "Epoch 569 loss:  2370.957763671875\n",
      "Epoch 570 loss:  2404.256103515625\n",
      "Epoch 571 loss:  2325.033935546875\n",
      "Epoch 572 loss:  2475.841552734375\n",
      "Epoch 573 loss:  2381.648193359375\n",
      "Epoch 574 loss:  2439.091796875\n",
      "Epoch 575 loss:  2393.604736328125\n",
      "Epoch 576 loss:  2269.609375\n",
      "Epoch 577 loss:  2213.184814453125\n",
      "Epoch 578 loss:  2070.175048828125\n",
      "Epoch 579 loss:  2038.9189453125\n",
      "Epoch 580 loss:  2132.969970703125\n",
      "Epoch 581 loss:  2108.41259765625\n",
      "Epoch 582 loss:  2006.0289306640625\n",
      "Epoch 583 loss:  1953.8082275390625\n",
      "Epoch 584 loss:  1981.1273193359375\n",
      "Epoch 585 loss:  2063.638671875\n",
      "Epoch 586 loss:  2038.422607421875\n",
      "Epoch 587 loss:  1986.19921875\n",
      "Epoch 588 loss:  1941.759765625\n",
      "Epoch 589 loss:  1902.819580078125\n",
      "Epoch 590 loss:  1911.2235107421875\n",
      "Epoch 591 loss:  1887.9652099609375\n",
      "Epoch 592 loss:  1877.2705078125\n",
      "Epoch 593 loss:  1870.5352783203125\n",
      "Epoch 594 loss:  1872.8516845703125\n",
      "Epoch 595 loss:  1871.672119140625\n",
      "Epoch 596 loss:  1860.068115234375\n",
      "Epoch 597 loss:  1836.05908203125\n",
      "Epoch 598 loss:  1852.6434326171875\n",
      "Epoch 599 loss:  1855.6744384765625\n",
      "Epoch 600 loss:  1832.4779052734375\n",
      "Epoch 601 loss:  1819.5767822265625\n",
      "Epoch 602 loss:  1825.1573486328125\n",
      "Epoch 603 loss:  1831.41015625\n",
      "Epoch 604 loss:  1813.3289794921875\n",
      "Epoch 605 loss:  1800.3773193359375\n",
      "Epoch 606 loss:  1802.599365234375\n",
      "Epoch 607 loss:  1799.503662109375\n",
      "Epoch 608 loss:  1791.2125244140625\n",
      "Epoch 609 loss:  1783.2373046875\n",
      "Epoch 610 loss:  1780.1458740234375\n",
      "Epoch 611 loss:  1777.772705078125\n",
      "Epoch 612 loss:  1772.4847412109375\n",
      "Epoch 613 loss:  1769.8929443359375\n",
      "Epoch 614 loss:  1765.3543701171875\n",
      "Epoch 615 loss:  1762.649658203125\n",
      "Epoch 616 loss:  1761.5655517578125\n",
      "Epoch 617 loss:  1758.810546875\n",
      "Epoch 618 loss:  1755.4737548828125\n",
      "Epoch 619 loss:  1750.8724365234375\n",
      "Epoch 620 loss:  1749.7689208984375\n",
      "Epoch 621 loss:  1747.6810302734375\n",
      "Epoch 622 loss:  1745.025390625\n",
      "Epoch 623 loss:  1743.104736328125\n",
      "Epoch 624 loss:  1740.643310546875\n",
      "Epoch 625 loss:  1739.058837890625\n",
      "Epoch 626 loss:  1735.680419921875\n",
      "Epoch 627 loss:  1732.9267578125\n",
      "Epoch 628 loss:  1730.718017578125\n",
      "Epoch 629 loss:  1729.092041015625\n",
      "Epoch 630 loss:  1727.2857666015625\n",
      "Epoch 631 loss:  1724.1815185546875\n",
      "Epoch 632 loss:  1722.475830078125\n",
      "Epoch 633 loss:  1720.6495361328125\n",
      "Epoch 634 loss:  1718.482177734375\n",
      "Epoch 635 loss:  1716.0147705078125\n",
      "Epoch 636 loss:  1714.5897216796875\n",
      "Epoch 637 loss:  1713.2493896484375\n",
      "Epoch 638 loss:  1711.201171875\n",
      "Epoch 639 loss:  1709.3116455078125\n",
      "Epoch 640 loss:  1707.755859375\n",
      "Epoch 641 loss:  1706.2557373046875\n",
      "Epoch 642 loss:  1704.4658203125\n",
      "Epoch 643 loss:  1702.7442626953125\n",
      "Epoch 644 loss:  1701.144775390625\n",
      "Epoch 645 loss:  1699.6353759765625\n",
      "Epoch 646 loss:  1697.8709716796875\n",
      "Epoch 647 loss:  1696.0926513671875\n",
      "Epoch 648 loss:  1694.409912109375\n",
      "Epoch 649 loss:  1692.9150390625\n",
      "Epoch 650 loss:  1691.16748046875\n",
      "Epoch 651 loss:  1689.5111083984375\n",
      "Epoch 652 loss:  1687.98388671875\n",
      "Epoch 653 loss:  1686.5211181640625\n",
      "Epoch 654 loss:  1684.9613037109375\n",
      "Epoch 655 loss:  1683.4803466796875\n",
      "Epoch 656 loss:  1682.082275390625\n",
      "Epoch 657 loss:  1680.7347412109375\n",
      "Epoch 658 loss:  1679.370361328125\n",
      "Epoch 659 loss:  1678.0128173828125\n",
      "Epoch 660 loss:  1676.655517578125\n",
      "Epoch 661 loss:  1675.3448486328125\n",
      "Epoch 662 loss:  1673.9669189453125\n",
      "Epoch 663 loss:  1672.5758056640625\n",
      "Epoch 664 loss:  1671.1925048828125\n",
      "Epoch 665 loss:  1669.8167724609375\n",
      "Epoch 666 loss:  1668.3931884765625\n",
      "Epoch 667 loss:  1666.9576416015625\n",
      "Epoch 668 loss:  1665.506591796875\n",
      "Epoch 669 loss:  1664.036376953125\n",
      "Epoch 670 loss:  1662.5533447265625\n",
      "Epoch 671 loss:  1661.0809326171875\n",
      "Epoch 672 loss:  1659.648193359375\n",
      "Epoch 673 loss:  1658.326904296875\n",
      "Epoch 674 loss:  1657.09375\n",
      "Epoch 675 loss:  1655.8839111328125\n",
      "Epoch 676 loss:  1654.635498046875\n",
      "Epoch 677 loss:  1653.3525390625\n",
      "Epoch 678 loss:  1652.0604248046875\n",
      "Epoch 679 loss:  1650.7926025390625\n",
      "Epoch 680 loss:  1649.5467529296875\n",
      "Epoch 681 loss:  1648.296630859375\n",
      "Epoch 682 loss:  1647.0299072265625\n",
      "Epoch 683 loss:  1645.736083984375\n",
      "Epoch 684 loss:  1644.405517578125\n",
      "Epoch 685 loss:  1643.0345458984375\n",
      "Epoch 686 loss:  1641.6356201171875\n",
      "Epoch 687 loss:  1640.2156982421875\n",
      "Epoch 688 loss:  1638.774169921875\n",
      "Epoch 689 loss:  1637.2967529296875\n",
      "Epoch 690 loss:  1635.724853515625\n",
      "Epoch 691 loss:  1633.695068359375\n",
      "Epoch 692 loss:  1626.6434326171875\n",
      "Epoch 693 loss:  1614.615478515625\n",
      "Epoch 694 loss:  1621.08154296875\n",
      "Epoch 695 loss:  1612.1214599609375\n",
      "Epoch 696 loss:  1610.3504638671875\n",
      "Epoch 697 loss:  1608.485595703125\n",
      "Epoch 698 loss:  1606.81982421875\n",
      "Epoch 699 loss:  1606.584716796875\n",
      "Epoch 700 loss:  1604.007568359375\n",
      "Epoch 701 loss:  1601.7855224609375\n",
      "Epoch 702 loss:  1600.0531005859375\n",
      "Epoch 703 loss:  1598.4744873046875\n",
      "Epoch 704 loss:  1597.1170654296875\n",
      "Epoch 705 loss:  1595.691650390625\n",
      "Epoch 706 loss:  1594.18017578125\n",
      "Epoch 707 loss:  1592.5030517578125\n",
      "Epoch 708 loss:  1590.6820068359375\n",
      "Epoch 709 loss:  1588.846435546875\n",
      "Epoch 710 loss:  1587.08251953125\n",
      "Epoch 711 loss:  1585.2796630859375\n",
      "Epoch 712 loss:  1583.3148193359375\n",
      "Epoch 713 loss:  1581.3604736328125\n",
      "Epoch 714 loss:  1579.527099609375\n",
      "Epoch 715 loss:  1577.8251953125\n",
      "Epoch 716 loss:  1576.2291259765625\n",
      "Epoch 717 loss:  1574.7835693359375\n",
      "Epoch 718 loss:  1573.3544921875\n",
      "Epoch 719 loss:  1571.72705078125\n",
      "Epoch 720 loss:  1569.997802734375\n",
      "Epoch 721 loss:  1568.29833984375\n",
      "Epoch 722 loss:  1566.635009765625\n",
      "Epoch 723 loss:  1564.95068359375\n",
      "Epoch 724 loss:  1563.1954345703125\n",
      "Epoch 725 loss:  1561.3148193359375\n",
      "Epoch 726 loss:  1559.3212890625\n",
      "Epoch 727 loss:  1557.243896484375\n",
      "Epoch 728 loss:  1555.13818359375\n",
      "Epoch 729 loss:  1553.006103515625\n",
      "Epoch 730 loss:  1550.8626708984375\n",
      "Epoch 731 loss:  1548.7188720703125\n",
      "Epoch 732 loss:  1546.5753173828125\n",
      "Epoch 733 loss:  1544.411376953125\n",
      "Epoch 734 loss:  1542.229736328125\n",
      "Epoch 735 loss:  1540.07861328125\n",
      "Epoch 736 loss:  1537.9864501953125\n",
      "Epoch 737 loss:  1535.960205078125\n",
      "Epoch 738 loss:  1533.990966796875\n",
      "Epoch 739 loss:  1532.087158203125\n",
      "Epoch 740 loss:  1530.2490234375\n",
      "Epoch 741 loss:  1528.4647216796875\n",
      "Epoch 742 loss:  1526.734130859375\n",
      "Epoch 743 loss:  1525.05908203125\n",
      "Epoch 744 loss:  1523.4427490234375\n",
      "Epoch 745 loss:  1521.8779296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 746 loss:  1520.3544921875\n",
      "Epoch 747 loss:  1518.8619384765625\n",
      "Epoch 748 loss:  1517.391845703125\n",
      "Epoch 749 loss:  1515.9381103515625\n",
      "Epoch 750 loss:  1514.4979248046875\n",
      "Epoch 751 loss:  1513.0677490234375\n",
      "Epoch 752 loss:  1511.64599609375\n",
      "Epoch 753 loss:  1510.231689453125\n",
      "Epoch 754 loss:  1508.82373046875\n",
      "Epoch 755 loss:  1507.4176025390625\n",
      "Epoch 756 loss:  1506.0113525390625\n",
      "Epoch 757 loss:  1504.6036376953125\n",
      "Epoch 758 loss:  1503.19384765625\n",
      "Epoch 759 loss:  1501.7774658203125\n",
      "Epoch 760 loss:  1500.3482666015625\n",
      "Epoch 761 loss:  1498.8980712890625\n",
      "Epoch 762 loss:  1497.4161376953125\n",
      "Epoch 763 loss:  1495.887939453125\n",
      "Epoch 764 loss:  1494.3011474609375\n",
      "Epoch 765 loss:  1492.6322021484375\n",
      "Epoch 766 loss:  1490.83544921875\n",
      "Epoch 767 loss:  1488.8460693359375\n",
      "Epoch 768 loss:  1486.5806884765625\n",
      "Epoch 769 loss:  1483.918212890625\n",
      "Epoch 770 loss:  1480.6927490234375\n",
      "Epoch 771 loss:  1476.7418212890625\n",
      "Epoch 772 loss:  1472.3082275390625\n",
      "Epoch 773 loss:  1467.22509765625\n",
      "Epoch 774 loss:  1461.6868896484375\n",
      "Epoch 775 loss:  1456.1488037109375\n",
      "Epoch 776 loss:  1450.29345703125\n",
      "Epoch 777 loss:  1445.1180419921875\n",
      "Epoch 778 loss:  1439.7701416015625\n",
      "Epoch 779 loss:  1435.2265625\n",
      "Epoch 780 loss:  1430.7618408203125\n",
      "Epoch 781 loss:  1427.164306640625\n",
      "Epoch 782 loss:  1423.5198974609375\n",
      "Epoch 783 loss:  1420.3486328125\n",
      "Epoch 784 loss:  1417.0703125\n",
      "Epoch 785 loss:  1414.3409423828125\n",
      "Epoch 786 loss:  1411.618408203125\n",
      "Epoch 787 loss:  1409.09228515625\n",
      "Epoch 788 loss:  1406.4827880859375\n",
      "Epoch 789 loss:  1404.8280029296875\n",
      "Epoch 790 loss:  1403.6746826171875\n",
      "Epoch 791 loss:  1403.9815673828125\n",
      "Epoch 792 loss:  1405.2161865234375\n",
      "Epoch 793 loss:  1413.29052734375\n",
      "Epoch 794 loss:  1423.813232421875\n",
      "Epoch 795 loss:  1449.144775390625\n",
      "Epoch 796 loss:  1458.909912109375\n",
      "Epoch 797 loss:  1490.08935546875\n",
      "Epoch 798 loss:  1466.6976318359375\n",
      "Epoch 799 loss:  1454.4642333984375\n",
      "Epoch 800 loss:  1403.53076171875\n",
      "Epoch 801 loss:  1381.9837646484375\n",
      "Epoch 802 loss:  1381.4652099609375\n",
      "Epoch 803 loss:  1396.05615234375\n",
      "Epoch 804 loss:  1415.0982666015625\n",
      "Epoch 805 loss:  1407.9041748046875\n",
      "Epoch 806 loss:  1393.9464111328125\n",
      "Epoch 807 loss:  1376.40283203125\n",
      "Epoch 808 loss:  1367.403076171875\n",
      "Epoch 809 loss:  1365.077392578125\n",
      "Epoch 810 loss:  1370.4906005859375\n",
      "Epoch 811 loss:  1378.562744140625\n",
      "Epoch 812 loss:  1382.9554443359375\n",
      "Epoch 813 loss:  1386.830810546875\n",
      "Epoch 814 loss:  1384.0986328125\n",
      "Epoch 815 loss:  1380.69140625\n",
      "Epoch 816 loss:  1371.382080078125\n",
      "Epoch 817 loss:  1363.8883056640625\n",
      "Epoch 818 loss:  1355.88671875\n",
      "Epoch 819 loss:  1350.5924072265625\n",
      "Epoch 820 loss:  1348.096923828125\n",
      "Epoch 821 loss:  1346.8319091796875\n",
      "Epoch 822 loss:  1347.4461669921875\n",
      "Epoch 823 loss:  1349.121826171875\n",
      "Epoch 824 loss:  1352.13525390625\n",
      "Epoch 825 loss:  1356.499755859375\n",
      "Epoch 826 loss:  1363.6036376953125\n",
      "Epoch 827 loss:  1370.70263671875\n",
      "Epoch 828 loss:  1381.8548583984375\n",
      "Epoch 829 loss:  1382.2955322265625\n",
      "Epoch 830 loss:  1383.700439453125\n",
      "Epoch 831 loss:  1369.251953125\n",
      "Epoch 832 loss:  1355.25341796875\n",
      "Epoch 833 loss:  1339.36376953125\n",
      "Epoch 834 loss:  1331.42919921875\n",
      "Epoch 835 loss:  1330.2379150390625\n",
      "Epoch 836 loss:  1334.449951171875\n",
      "Epoch 837 loss:  1340.6279296875\n",
      "Epoch 838 loss:  1344.8492431640625\n",
      "Epoch 839 loss:  1347.68701171875\n",
      "Epoch 840 loss:  1344.5303955078125\n",
      "Epoch 841 loss:  1340.888427734375\n",
      "Epoch 842 loss:  1333.8565673828125\n",
      "Epoch 843 loss:  1328.05126953125\n",
      "Epoch 844 loss:  1322.3218994140625\n",
      "Epoch 845 loss:  1318.372314453125\n",
      "Epoch 846 loss:  1315.926025390625\n",
      "Epoch 847 loss:  1314.8883056640625\n",
      "Epoch 848 loss:  1314.8804931640625\n",
      "Epoch 849 loss:  1315.583251953125\n",
      "Epoch 850 loss:  1316.7943115234375\n",
      "Epoch 851 loss:  1318.110107421875\n",
      "Epoch 852 loss:  1320.0045166015625\n",
      "Epoch 853 loss:  1321.3807373046875\n",
      "Epoch 854 loss:  1323.596435546875\n",
      "Epoch 855 loss:  1324.3671875\n",
      "Epoch 856 loss:  1326.1937255859375\n",
      "Epoch 857 loss:  1325.484619140625\n",
      "Epoch 858 loss:  1325.9749755859375\n",
      "Epoch 859 loss:  1323.432861328125\n",
      "Epoch 860 loss:  1321.85205078125\n",
      "Epoch 861 loss:  1317.8907470703125\n",
      "Epoch 862 loss:  1314.832275390625\n",
      "Epoch 863 loss:  1310.9478759765625\n",
      "Epoch 864 loss:  1307.9654541015625\n",
      "Epoch 865 loss:  1305.3125\n",
      "Epoch 866 loss:  1303.408447265625\n",
      "Epoch 867 loss:  1302.0601806640625\n",
      "Epoch 868 loss:  1301.1800537109375\n",
      "Epoch 869 loss:  1300.637451171875\n",
      "Epoch 870 loss:  1300.3411865234375\n",
      "Epoch 871 loss:  1300.2664794921875\n",
      "Epoch 872 loss:  1300.3917236328125\n",
      "Epoch 873 loss:  1300.88818359375\n",
      "Epoch 874 loss:  1301.671142578125\n",
      "Epoch 875 loss:  1303.295654296875\n",
      "Epoch 876 loss:  1305.123779296875\n",
      "Epoch 877 loss:  1308.6436767578125\n",
      "Epoch 878 loss:  1311.391845703125\n",
      "Epoch 879 loss:  1316.9010009765625\n",
      "Epoch 880 loss:  1318.5101318359375\n",
      "Epoch 881 loss:  1322.643310546875\n",
      "Epoch 882 loss:  1318.6650390625\n",
      "Epoch 883 loss:  1316.444091796875\n",
      "Epoch 884 loss:  1307.833984375\n",
      "Epoch 885 loss:  1301.4014892578125\n",
      "Epoch 886 loss:  1294.37939453125\n",
      "Epoch 887 loss:  1289.753662109375\n",
      "Epoch 888 loss:  1286.8262939453125\n",
      "Epoch 889 loss:  1285.515869140625\n",
      "Epoch 890 loss:  1285.3927001953125\n",
      "Epoch 891 loss:  1286.151611328125\n",
      "Epoch 892 loss:  1287.8203125\n",
      "Epoch 893 loss:  1290.0958251953125\n",
      "Epoch 894 loss:  1293.8909912109375\n",
      "Epoch 895 loss:  1296.6839599609375\n",
      "Epoch 896 loss:  1304.347412109375\n",
      "Epoch 897 loss:  1309.3233642578125\n",
      "Epoch 898 loss:  1319.404052734375\n",
      "Epoch 899 loss:  1315.3692626953125\n",
      "Epoch 900 loss:  1313.9344482421875\n",
      "Epoch 901 loss:  1299.26416015625\n",
      "Epoch 902 loss:  1288.3785400390625\n",
      "Epoch 903 loss:  1278.10302734375\n",
      "Epoch 904 loss:  1272.8564453125\n",
      "Epoch 905 loss:  1271.83056640625\n",
      "Epoch 906 loss:  1274.0946044921875\n",
      "Epoch 907 loss:  1279.095703125\n",
      "Epoch 908 loss:  1285.275634765625\n",
      "Epoch 909 loss:  1295.6602783203125\n",
      "Epoch 910 loss:  1302.8857421875\n",
      "Epoch 911 loss:  1316.447509765625\n",
      "Epoch 912 loss:  1314.2177734375\n",
      "Epoch 913 loss:  1315.329833984375\n",
      "Epoch 914 loss:  1298.017578125\n",
      "Epoch 915 loss:  1284.5255126953125\n",
      "Epoch 916 loss:  1271.6268310546875\n",
      "Epoch 917 loss:  1266.2115478515625\n",
      "Epoch 918 loss:  1267.51220703125\n",
      "Epoch 919 loss:  1273.009765625\n",
      "Epoch 920 loss:  1281.0804443359375\n",
      "Epoch 921 loss:  1285.8797607421875\n",
      "Epoch 922 loss:  1292.6416015625\n",
      "Epoch 923 loss:  1290.525146484375\n",
      "Epoch 924 loss:  1291.2303466796875\n",
      "Epoch 925 loss:  1283.4691162109375\n",
      "Epoch 926 loss:  1278.38427734375\n",
      "Epoch 927 loss:  1270.3023681640625\n",
      "Epoch 928 loss:  1264.633056640625\n",
      "Epoch 929 loss:  1259.88720703125\n",
      "Epoch 930 loss:  1257.0389404296875\n",
      "Epoch 931 loss:  1255.56103515625\n",
      "Epoch 932 loss:  1255.0858154296875\n",
      "Epoch 933 loss:  1255.390380859375\n",
      "Epoch 934 loss:  1256.3369140625\n",
      "Epoch 935 loss:  1258.26318359375\n",
      "Epoch 936 loss:  1260.6180419921875\n",
      "Epoch 937 loss:  1265.3070068359375\n",
      "Epoch 938 loss:  1269.5318603515625\n",
      "Epoch 939 loss:  1278.905517578125\n",
      "Epoch 940 loss:  1283.5675048828125\n",
      "Epoch 941 loss:  1295.623046875\n",
      "Epoch 942 loss:  1291.946044921875\n",
      "Epoch 943 loss:  1293.531005859375\n",
      "Epoch 944 loss:  1277.29296875\n",
      "Epoch 945 loss:  1265.3599853515625\n",
      "Epoch 946 loss:  1251.416259765625\n",
      "Epoch 947 loss:  1243.1702880859375\n",
      "Epoch 948 loss:  1239.736572265625\n",
      "Epoch 949 loss:  1240.3863525390625\n",
      "Epoch 950 loss:  1244.4849853515625\n",
      "Epoch 951 loss:  1251.3134765625\n",
      "Epoch 952 loss:  1265.849365234375\n",
      "Epoch 953 loss:  1284.1787109375\n",
      "Epoch 954 loss:  1324.739990234375\n",
      "Epoch 955 loss:  1353.9757080078125\n",
      "Epoch 956 loss:  1381.7301025390625\n",
      "Epoch 957 loss:  1328.8563232421875\n",
      "Epoch 958 loss:  1272.1658935546875\n",
      "Epoch 959 loss:  1231.475341796875\n",
      "Epoch 960 loss:  1228.24072265625\n",
      "Epoch 961 loss:  1252.5687255859375\n",
      "Epoch 962 loss:  1280.052734375\n",
      "Epoch 963 loss:  1300.2091064453125\n",
      "Epoch 964 loss:  1298.443359375\n",
      "Epoch 965 loss:  1258.122802734375\n",
      "Epoch 966 loss:  1234.9237060546875\n",
      "Epoch 967 loss:  1225.5826416015625\n",
      "Epoch 968 loss:  1238.538330078125\n",
      "Epoch 969 loss:  1273.9854736328125\n",
      "Epoch 970 loss:  1273.249755859375\n",
      "Epoch 971 loss:  1241.9942626953125\n",
      "Epoch 972 loss:  1232.8973388671875\n",
      "Epoch 973 loss:  1225.814453125\n",
      "Epoch 974 loss:  1210.8695068359375\n",
      "Epoch 975 loss:  1231.7982177734375\n",
      "Epoch 976 loss:  1221.9140625\n",
      "Epoch 977 loss:  1228.8428955078125\n",
      "Epoch 978 loss:  1249.7183837890625\n",
      "Epoch 979 loss:  1230.2066650390625\n",
      "Epoch 980 loss:  1239.6871337890625\n",
      "Epoch 981 loss:  1215.5699462890625\n",
      "Epoch 982 loss:  1211.6932373046875\n",
      "Epoch 983 loss:  1207.444091796875\n",
      "Epoch 984 loss:  1197.8658447265625\n",
      "Epoch 985 loss:  1202.5216064453125\n",
      "Epoch 986 loss:  1196.312744140625\n",
      "Epoch 987 loss:  1194.0205078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 988 loss:  1197.515625\n",
      "Epoch 989 loss:  1190.0186767578125\n",
      "Epoch 990 loss:  1189.7811279296875\n",
      "Epoch 991 loss:  1189.118896484375\n",
      "Epoch 992 loss:  1184.5989990234375\n",
      "Epoch 993 loss:  1185.5382080078125\n",
      "Epoch 994 loss:  1183.0181884765625\n",
      "Epoch 995 loss:  1182.2115478515625\n",
      "Epoch 996 loss:  1182.1405029296875\n",
      "Epoch 997 loss:  1180.2120361328125\n",
      "Epoch 998 loss:  1180.5791015625\n",
      "Epoch 999 loss:  1181.13037109375\n",
      "Epoch 1000 loss:  1184.299560546875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "running_loss = 0.0\n",
    "\n",
    "#inputs = Variable(torch.tensor(x_train).float())\n",
    "#labels = Variable(torch.tensor(y_train).float())\n",
    "\n",
    "batch_no\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i in range(batch_no):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs = Variable(torch.tensor(x_train[start:end]).float())\n",
    "        labels = Variable(torch.tensor(y_train[start:end]).float())\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print(\"outputs\",outputs)\n",
    "        #print(\"outputs\",outputs,outputs.shape,\"labels\",labels, labels.shape)\n",
    "        loss = criterion(outputs, torch.unsqueeze(labels, dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print('Epoch {}'.format(epoch+1), \"loss: \",running_loss)\n",
    "    running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253 253\n",
      "0.9412235092713305\n",
      "4.7024455087249555\n",
      "22.636758893280636\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "X = Variable(torch.tensor(x_train).float()) \n",
    "result = net(X)\n",
    "pred=result.data[:,0].numpy()\n",
    "print(len(pred),len(y_train))\n",
    "print (r2_score(pred,y_train))\n",
    "print (mean_squared_error(pred,y_train))\n",
    "print (np.mean(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3162933756711457"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sqrt((pred - y_train)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sq(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253 253\n",
      "0.7635263966826227\n",
      "16.128128208921893\n",
      "22.231620553359686\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "X = Variable(torch.tensor(x_test).float()) \n",
    "result = net(X)\n",
    "pred=result.data[:,0].numpy()\n",
    "print(len(pred),len(y_test))\n",
    "print (r2_score(pred,y_test))\n",
    "print (mean_squared_error(pred,y_test))\n",
    "print (np.mean(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "Then we define the sizes of all the layers and the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_h1, n_out, batch_size = 13, 5, 1, len(tensor_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature, size_hidden, n_output = 13, 5, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_no = len(x_train)\n",
    "batch_size = 50\n",
    "num_epochs = 200\n",
    "learning_rate = 0.01\n",
    "size_hidden= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the model on : cpu\n"
     ]
    }
   ],
   "source": [
    "#Create the model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "print(\"Executing the model on :\", device)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, size_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, size_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(size_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "net = Net(n_feature, size_hidden, n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss:  189120.421875\n",
      "Epoch 2 loss:  4.15542021528299e+23\n",
      "Epoch 3 loss:  inf\n",
      "Epoch 4 loss:  nan\n",
      "Epoch 5 loss:  nan\n",
      "Epoch 6 loss:  nan\n",
      "Epoch 7 loss:  nan\n",
      "Epoch 8 loss:  nan\n",
      "Epoch 9 loss:  nan\n",
      "Epoch 10 loss:  nan\n",
      "Epoch 11 loss:  nan\n",
      "Epoch 12 loss:  nan\n",
      "Epoch 13 loss:  nan\n",
      "Epoch 14 loss:  nan\n",
      "Epoch 15 loss:  nan\n",
      "Epoch 16 loss:  nan\n",
      "Epoch 17 loss:  nan\n",
      "Epoch 18 loss:  nan\n",
      "Epoch 19 loss:  nan\n",
      "Epoch 20 loss:  nan\n",
      "Epoch 21 loss:  nan\n",
      "Epoch 22 loss:  nan\n",
      "Epoch 23 loss:  nan\n",
      "Epoch 24 loss:  nan\n",
      "Epoch 25 loss:  nan\n",
      "Epoch 26 loss:  nan\n",
      "Epoch 27 loss:  nan\n",
      "Epoch 28 loss:  nan\n",
      "Epoch 29 loss:  nan\n",
      "Epoch 30 loss:  nan\n",
      "Epoch 31 loss:  nan\n",
      "Epoch 32 loss:  nan\n",
      "Epoch 33 loss:  nan\n",
      "Epoch 34 loss:  nan\n",
      "Epoch 35 loss:  nan\n",
      "Epoch 36 loss:  nan\n",
      "Epoch 37 loss:  nan\n",
      "Epoch 38 loss:  nan\n",
      "Epoch 39 loss:  nan\n",
      "Epoch 40 loss:  nan\n",
      "Epoch 41 loss:  nan\n",
      "Epoch 42 loss:  nan\n",
      "Epoch 43 loss:  nan\n",
      "Epoch 44 loss:  nan\n",
      "Epoch 45 loss:  nan\n",
      "Epoch 46 loss:  nan\n",
      "Epoch 47 loss:  nan\n",
      "Epoch 48 loss:  nan\n",
      "Epoch 49 loss:  nan\n",
      "Epoch 50 loss:  nan\n",
      "Epoch 51 loss:  nan\n",
      "Epoch 52 loss:  nan\n",
      "Epoch 53 loss:  nan\n",
      "Epoch 54 loss:  nan\n",
      "Epoch 55 loss:  nan\n",
      "Epoch 56 loss:  nan\n",
      "Epoch 57 loss:  nan\n",
      "Epoch 58 loss:  nan\n",
      "Epoch 59 loss:  nan\n",
      "Epoch 60 loss:  nan\n",
      "Epoch 61 loss:  nan\n",
      "Epoch 62 loss:  nan\n",
      "Epoch 63 loss:  nan\n",
      "Epoch 64 loss:  nan\n",
      "Epoch 65 loss:  nan\n",
      "Epoch 66 loss:  nan\n",
      "Epoch 67 loss:  nan\n",
      "Epoch 68 loss:  nan\n",
      "Epoch 69 loss:  nan\n",
      "Epoch 70 loss:  nan\n",
      "Epoch 71 loss:  nan\n",
      "Epoch 72 loss:  nan\n",
      "Epoch 73 loss:  nan\n",
      "Epoch 74 loss:  nan\n",
      "Epoch 75 loss:  nan\n",
      "Epoch 76 loss:  nan\n",
      "Epoch 77 loss:  nan\n",
      "Epoch 78 loss:  nan\n",
      "Epoch 79 loss:  nan\n",
      "Epoch 80 loss:  nan\n",
      "Epoch 81 loss:  nan\n",
      "Epoch 82 loss:  nan\n",
      "Epoch 83 loss:  nan\n",
      "Epoch 84 loss:  nan\n",
      "Epoch 85 loss:  nan\n",
      "Epoch 86 loss:  nan\n",
      "Epoch 87 loss:  nan\n",
      "Epoch 88 loss:  nan\n",
      "Epoch 89 loss:  nan\n",
      "Epoch 90 loss:  nan\n",
      "Epoch 91 loss:  nan\n",
      "Epoch 92 loss:  nan\n",
      "Epoch 93 loss:  nan\n",
      "Epoch 94 loss:  nan\n",
      "Epoch 95 loss:  nan\n",
      "Epoch 96 loss:  nan\n",
      "Epoch 97 loss:  nan\n",
      "Epoch 98 loss:  nan\n",
      "Epoch 99 loss:  nan\n",
      "Epoch 100 loss:  nan\n",
      "Epoch 101 loss:  nan\n",
      "Epoch 102 loss:  nan\n",
      "Epoch 103 loss:  nan\n",
      "Epoch 104 loss:  nan\n",
      "Epoch 105 loss:  nan\n",
      "Epoch 106 loss:  nan\n",
      "Epoch 107 loss:  nan\n",
      "Epoch 108 loss:  nan\n",
      "Epoch 109 loss:  nan\n",
      "Epoch 110 loss:  nan\n",
      "Epoch 111 loss:  nan\n",
      "Epoch 112 loss:  nan\n",
      "Epoch 113 loss:  nan\n",
      "Epoch 114 loss:  nan\n",
      "Epoch 115 loss:  nan\n",
      "Epoch 116 loss:  nan\n",
      "Epoch 117 loss:  nan\n",
      "Epoch 118 loss:  nan\n",
      "Epoch 119 loss:  nan\n",
      "Epoch 120 loss:  nan\n",
      "Epoch 121 loss:  nan\n",
      "Epoch 122 loss:  nan\n",
      "Epoch 123 loss:  nan\n",
      "Epoch 124 loss:  nan\n",
      "Epoch 125 loss:  nan\n",
      "Epoch 126 loss:  nan\n",
      "Epoch 127 loss:  nan\n",
      "Epoch 128 loss:  nan\n",
      "Epoch 129 loss:  nan\n",
      "Epoch 130 loss:  nan\n",
      "Epoch 131 loss:  nan\n",
      "Epoch 132 loss:  nan\n",
      "Epoch 133 loss:  nan\n",
      "Epoch 134 loss:  nan\n",
      "Epoch 135 loss:  nan\n",
      "Epoch 136 loss:  nan\n",
      "Epoch 137 loss:  nan\n",
      "Epoch 138 loss:  nan\n",
      "Epoch 139 loss:  nan\n",
      "Epoch 140 loss:  nan\n",
      "Epoch 141 loss:  nan\n",
      "Epoch 142 loss:  nan\n",
      "Epoch 143 loss:  nan\n",
      "Epoch 144 loss:  nan\n",
      "Epoch 145 loss:  nan\n",
      "Epoch 146 loss:  nan\n",
      "Epoch 147 loss:  nan\n",
      "Epoch 148 loss:  nan\n",
      "Epoch 149 loss:  nan\n",
      "Epoch 150 loss:  nan\n",
      "Epoch 151 loss:  nan\n",
      "Epoch 152 loss:  nan\n",
      "Epoch 153 loss:  nan\n",
      "Epoch 154 loss:  nan\n",
      "Epoch 155 loss:  nan\n",
      "Epoch 156 loss:  nan\n",
      "Epoch 157 loss:  nan\n",
      "Epoch 158 loss:  nan\n",
      "Epoch 159 loss:  nan\n",
      "Epoch 160 loss:  nan\n",
      "Epoch 161 loss:  nan\n",
      "Epoch 162 loss:  nan\n",
      "Epoch 163 loss:  nan\n",
      "Epoch 164 loss:  nan\n",
      "Epoch 165 loss:  nan\n",
      "Epoch 166 loss:  nan\n",
      "Epoch 167 loss:  nan\n",
      "Epoch 168 loss:  nan\n",
      "Epoch 169 loss:  nan\n",
      "Epoch 170 loss:  nan\n",
      "Epoch 171 loss:  nan\n",
      "Epoch 172 loss:  nan\n",
      "Epoch 173 loss:  nan\n",
      "Epoch 174 loss:  nan\n",
      "Epoch 175 loss:  nan\n",
      "Epoch 176 loss:  nan\n",
      "Epoch 177 loss:  nan\n",
      "Epoch 178 loss:  nan\n",
      "Epoch 179 loss:  nan\n",
      "Epoch 180 loss:  nan\n",
      "Epoch 181 loss:  nan\n",
      "Epoch 182 loss:  nan\n",
      "Epoch 183 loss:  nan\n",
      "Epoch 184 loss:  nan\n",
      "Epoch 185 loss:  nan\n",
      "Epoch 186 loss:  nan\n",
      "Epoch 187 loss:  nan\n",
      "Epoch 188 loss:  nan\n",
      "Epoch 189 loss:  nan\n",
      "Epoch 190 loss:  nan\n",
      "Epoch 191 loss:  nan\n",
      "Epoch 192 loss:  nan\n",
      "Epoch 193 loss:  nan\n",
      "Epoch 194 loss:  nan\n",
      "Epoch 195 loss:  nan\n",
      "Epoch 196 loss:  nan\n",
      "Epoch 197 loss:  nan\n",
      "Epoch 198 loss:  nan\n",
      "Epoch 199 loss:  nan\n",
      "Epoch 200 loss:  nan\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "running_loss = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    # Mini batch learning\n",
    "    #for i in range(50):\n",
    "    inputs = Variable(torch.FloatTensor(x_train))\n",
    "    labels = Variable(torch.FloatTensor(y_train))\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs)\n",
    "    #print(\"outputs\",outputs)\n",
    "    #print(\"outputs\",outputs,outputs.shape,\"labels\",labels, labels.shape)\n",
    "    loss = criterion(outputs, torch.unsqueeze(labels, dim=1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "        \n",
    "    print('Epoch {}'.format(epoch+1), \"loss: \",running_loss)\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354 354\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b4709e05f7b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \"\"\"\n\u001b[1;32m    533\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 534\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    535\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \"\"\"\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 573\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = Variable(torch.FloatTensor(x_train)) \n",
    "result = net(X)\n",
    "pred=result.data[:,0].numpy()\n",
    "print(len(pred),len(y_train))\n",
    "r2_score(pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15.0000, 26.6000, 45.4000, 20.8000, 34.9000, 21.9000, 28.7000,  7.2000,\n",
       "        20.0000, 32.2000, 24.1000, 18.5000, 13.5000, 27.0000, 23.1000, 18.9000,\n",
       "        24.5000, 43.1000, 19.8000, 13.8000, 15.6000, 50.0000, 37.2000, 46.0000,\n",
       "        50.0000, 21.2000, 14.9000, 19.6000, 19.4000, 18.6000, 26.5000, 32.0000,\n",
       "        10.9000, 20.0000, 21.4000, 31.0000, 25.0000, 15.4000, 13.1000, 37.6000,\n",
       "        37.0000, 18.9000, 27.9000, 50.0000, 14.4000, 22.0000, 19.9000, 21.6000,\n",
       "        15.6000, 15.0000, 32.4000, 29.6000, 20.4000, 12.3000, 19.1000, 14.9000,\n",
       "        17.8000,  8.8000, 35.4000, 11.5000, 19.6000, 20.6000, 15.6000, 19.9000,\n",
       "        23.3000, 22.3000, 24.8000, 16.1000, 22.8000, 30.5000, 20.4000, 24.4000,\n",
       "        16.6000, 26.2000, 16.4000, 20.1000, 13.9000, 19.4000, 22.8000, 13.8000,\n",
       "        31.6000, 10.5000, 23.8000, 22.4000, 19.3000, 22.2000, 12.6000, 19.4000,\n",
       "        22.2000, 29.8000,  9.6000, 34.9000, 21.4000, 25.3000, 32.9000, 26.6000,\n",
       "        14.6000, 31.5000, 23.3000, 33.3000, 17.5000, 19.1000, 48.5000, 17.1000,\n",
       "        23.1000, 28.4000, 18.9000, 13.0000, 17.2000, 24.1000, 18.5000, 21.8000,\n",
       "        13.3000, 23.0000, 14.1000, 23.9000, 24.0000, 17.2000, 21.5000, 19.1000,\n",
       "        20.8000, 36.0000, 20.1000,  8.7000, 13.6000, 22.0000, 22.2000, 21.1000,\n",
       "        13.4000, 17.4000, 20.1000, 10.2000, 23.1000, 10.2000, 13.1000, 14.3000,\n",
       "        14.5000,  7.2000, 19.6000, 20.6000, 22.7000, 26.4000,  7.5000, 20.3000,\n",
       "        50.0000,  8.5000, 20.3000, 16.1000, 22.0000, 19.6000, 10.2000, 23.2000,\n",
       "        35.2000, 25.0000, 36.2000, 16.1000, 10.9000, 36.4000, 25.0000, 20.1000,\n",
       "        16.8000, 23.7000, 42.3000, 17.9000, 12.7000, 50.0000, 18.4000, 33.4000,\n",
       "        22.9000, 14.6000, 29.9000, 22.6000, 22.5000, 29.0000, 50.0000, 37.9000,\n",
       "        21.4000, 29.4000, 20.3000, 23.0000, 30.1000, 21.7000, 36.5000, 25.0000,\n",
       "        24.5000, 37.3000, 33.8000, 24.7000, 32.7000, 23.1000, 25.1000, 21.7000,\n",
       "        13.4000, 24.8000, 12.7000, 11.8000,  8.3000, 20.2000, 41.3000, 23.2000,\n",
       "        23.1000, 24.3000, 19.3000, 10.8000, 18.6000, 29.0000, 23.9000, 19.5000,\n",
       "        13.1000, 31.7000, 21.0000, 18.2000, 21.0000, 21.2000, 14.1000, 33.2000,\n",
       "        13.8000, 19.9000, 21.7000, 20.6000, 21.2000, 13.6000, 18.9000, 18.0000,\n",
       "        24.1000, 28.7000, 23.4000, 15.2000, 23.6000, 13.8000, 11.7000, 16.3000,\n",
       "        50.0000, 13.5000, 50.0000, 31.5000, 22.6000, 12.1000, 21.7000, 14.1000,\n",
       "        22.4000, 13.4000, 33.1000, 20.6000,  8.3000, 36.2000,  6.3000, 21.5000,\n",
       "        23.3000, 24.0000, 19.1000, 29.6000, 27.9000, 16.2000,  9.5000, 24.6000,\n",
       "        15.6000,  8.1000, 15.3000, 19.0000, 22.0000, 28.0000, 19.2000, 14.5000,\n",
       "         9.7000, 30.7000, 20.6000, 16.0000, 19.8000, 17.8000, 21.2000, 28.7000,\n",
       "        41.7000,  7.2000, 13.4000, 17.7000, 26.7000, 23.8000, 21.8000, 27.1000,\n",
       "        18.3000,  5.0000, 10.4000, 18.5000, 17.4000, 28.6000, 50.0000, 14.2000,\n",
       "        31.2000,  8.4000, 23.0000, 35.1000, 23.9000, 20.3000, 46.7000, 15.0000,\n",
       "        18.4000, 17.8000, 22.5000, 18.8000, 20.9000, 50.0000, 29.1000, 17.1000,\n",
       "        10.5000, 18.8000, 27.5000, 19.5000, 22.0000,  5.0000, 21.7000, 50.0000,\n",
       "        35.4000, 32.0000, 20.5000, 16.8000, 22.9000,  8.5000, 50.0000, 22.2000,\n",
       "        15.2000, 22.6000,  7.0000, 19.3000, 26.4000, 12.8000, 19.2000, 19.7000,\n",
       "        38.7000, 30.3000, 25.0000, 23.7000, 11.7000, 17.5000,  5.6000, 25.0000,\n",
       "        14.9000, 22.3000, 20.5000, 50.0000, 24.8000, 20.7000, 23.2000, 19.7000,\n",
       "        19.4000, 29.8000, 34.9000, 11.0000, 12.5000, 23.5000, 24.4000, 16.5000,\n",
       "        20.8000, 23.3000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813],\n",
       "        [22.3813]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tensor_x_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
